# Alibaba PAI Python SDK

PAI Python SDK is provided by team *Alibaba Cloud PAI*. It provide convenience for user to access [PAI service in Alibaba Cloud](https://www.aliyun.com/product/bigdata/product/learn).

Currently, SDK support PAI pipeline service (PAIFlow), more ability, such as EAS, Blade service support will be included soon.

## Installation

```bash
pip install alipai --user
```

## Usage

### Setup PAI Session

Before access PAI service, user should initialize default PAI session.

```python

from pai.session import set_default_pai_session

session = set_default_pai_session(access_key_id="your_access_key", access_key_secret="your_access_secret", region_id="your_region_id", oss_bucket=oss_bucket)

```


### Access Pipeline Service

#### Use PipelineTemplate

PipelineTemplate object include the definition of "Workflow" use in PAI pipeline service. It could be fetch from remote pipeline service or construct from local Pipeline/Component. Saved pipeline template has unique `pipeline_id` which is generated by pipeline service.


```python

from pai.pipeline import PipelineTemplate
from pai.common import ProviderAlibabaPAI

# search PipelineTemplate which provide by `PAI` and include `xflow` in identifier.
templates, count = PipelineTemplate.list_templates(identifie="xflow", provider=ProviderAlibabaPAI)

# view template inputs/outputs (print the property to view the inputs/outputs spec if run in script mode).
templates[0]
templates[0].inputs
templates[0].outputs

# Get specific template by Identifier-Provider-Version
template = PipelineTemplate.get_by_identifier(identifier="split-xflow-maxCompute", provider=ProviderAlibabaPAI, version="v1")

# run pipeline use provided arguments.
job = template.run(job_name="demo-split-job", arguments={"inputArtifact": "odps://pai_online_project/tables/mnist_data", "fraction": 0.7}, wait=True, log_outputs=True)

# get pipeline run outputs.
job.get_outputs()

```


### Composite pipeline support

PAI Pipeline Service support user-defined workflow. Saved pipeline template (Local pipeline template as step will support in next release.) could be use as step to build a new pipeline. Assembled pipeline is runnable by provided require arguments and reusable by save to pipeline service.

```python

from pai.common import ProviderAlibabaPAI
from pai.pipeline import PipelineStep, Pipeline, PipelineTemplate
from pai.pipeline.types import PipelineParameter
from pai.session import get_default_xflow_execution
from pai.utils import gen_temp_table

# Definite the inputs parameters
def create_composite_pipeline():
    execution_input = PipelineParameter(name="execution", typ=dict)
    cols_to_double_input = PipelineParameter(name="cols_to_double")
    table_input = PipelineParameter(name="table_name")

    data_source_step = PipelineStep(
        identifier="dataSource-xflow-maxCompute", provider=ProviderAlibabaPAI,
        version="v1", name="dataSource", inputs={
            "execution": execution_input, "tableName": table_input, "partition": "",
        }
    )

    type_transform_step = PipelineStep(
        identifier="type-transform-xflow-maxCompute", provider=ProviderAlibabaPAI,
        version="v1", name="typeTransform", inputs={
            "inputArtifact": data_source_step.outputs["outputArtifact"],
            "execution": execution_input, "outputTable": gen_temp_table(),
            "cols_to_double": cols_to_double_input,
        }
    )

    split_template = PipelineTemplate.get_by_identifier(identifier="split-xflow-maxCompute", provider=ProviderAlibabaPAI, version="v1")
    split_step = split_template.as_step(inputs={"inputArtifact": type_transform_step.outputs[0],
            "execution": execution_input, "output1TableName": gen_temp_table(),
            "fraction": 0.5, "output2TableName": gen_temp_table(),
        })
    p = Pipeline(
        inputs=[execution_input, cols_to_double_input, table_input],
        steps=[data_source_step, type_transform_step, split_step],
        outputs=split_step.outputs[:1] + data_source_step.outputs[:1] + split_step.outputs[-1:],
    )
    return p

p = create_composite_pipeline()

# Draw the pipeline DAG graph constructed by pipeline step.
p.dot()

# Pipeline Manifest in Json
p.to_dict()

# Run pipeline
pipeline_run = p.run(job_name="demo-composite-pipeline-run", arguments={
            "execution": get_default_xflow_execution(),
            "cols_to_double": "time,hour,pm2,pm10,so2,co,no2",
            "table_name": "wumai_data",
        }, wait=True, log_outputs=True)

pipeline_id = p.save(identifier="demo-composite-pipeline", version="v1")

saved_template = PipelineTemplate.get(pipeline_id)

```


