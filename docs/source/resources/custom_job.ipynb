{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 使用CustomJob在PAI-DLC运行自定义任务\n",
    "\n",
    "以下例子展示了如何通过PAI SDK，使用本地算法脚本和自定义镜像在PAI-DLC上执行一个自定义作业。同时这个自定义作业可以作为一个节点任务，用于构建工作流，提交到PAI的Workflow中执行。\n",
    "\n",
    "## 准备工作.\n",
    "\n",
    "请首先安装PAI SDK，以支持运行以下的示例代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\r\n",
      "Collecting alipai==0.3.4b1\r\n",
      "\u001B[31m  ERROR: HTTP error 404 while getting https://pai-sdk.oss-cn-shanghai.aliyuncs.com/alipai/dist/alipai-0.3.4b1-py2.py3-none-any.whl\u001B[0m\r\n",
      "\u001B[31mERROR: Could not install requirement alipai==0.3.4b1 from https://pai-sdk.oss-cn-shanghai.aliyuncs.com/alipai/dist/alipai-0.3.4b1-py2.py3-none-any.whl because of HTTP error 404 Client Error: Not Found for url: https://pai-sdk.oss-cn-shanghai.aliyuncs.com/alipai/dist/alipai-0.3.4b1-py2.py3-none-any.whl for URL https://pai-sdk.oss-cn-shanghai.aliyuncs.com/alipai/dist/alipai-0.3.4b1-py2.py3-none-any.whl\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install https://pai-sdk.oss-cn-shanghai.aliyuncs.com/alipai/dist/alipai-0.3.4a1-py2.py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.4.dev5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "import pai\n",
    "\n",
    "print(pai.__version__)\n",
    "\n",
    "from pai.core.session import setup_default_session, Session\n",
    "from pai.job.common import JobConfig\n",
    "import oss2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 初始化默认的Session\n",
    "\n",
    "请在阿里云的控制台，获取使用的鉴权凭证和工作空间\n",
    "\n",
    "- AccessKeyId和AccessKeySecret\n",
    "\n",
    "请通过 [RAM控制台](https://ram.console.aliyun.com/manage/ak?spm=a2c8b.12215454.top-nav.dak.1704336aEeHgvy) 获取当前账号使用的AK信息\n",
    "\n",
    "- WorkspaceId\n",
    "\n",
    "通过 [PAI的控制台](https://pai.console.aliyun.com/?spm=a2c4g.11186623.0.0.506a7ba7JBg0qi&regionId=cn-hangzhou#/workspace/list) 查看你所在的AI工作空间ID.\n",
    "\n",
    "- OSS Bucket Name\n",
    "\n",
    "通过 [OSS控制台](https://oss.console.aliyun.com/) 查看可用的OSS Bucket，请确认使用的OSS region和工作空间是一致的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sess = Session.current()\n",
    "\n",
    "if not sess:\n",
    "    print(\"config session\")\n",
    "    sess = setup_default_session(\n",
    "        access_key_id=\"<YourAccessKeyId>\",\n",
    "        access_key_secret=\"<YourAccessKeySecret>\",\n",
    "        region_id=\"<RegionIdWorking>\",\n",
    "        workspace_id=\"<YourWorkspaceId>\",\n",
    "        oss_bucket_name=\"<YourOssBucketName>\",\n",
    "    )\n",
    "    # 将当前的配置持久化到 ~/.pai/config.json，SDK默认从对应的路径读取配置初始化默认session。\n",
    "    sess.persist_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 准备训练数据\n",
    "这里我们使用sklearn上的IRIS鸢尾花数据集作为训练数据，获得的数据集导出为CSV后，上传到OSS，供后续的训练作业使用。\n",
    "\n",
    "- job_train_data_uri: 作业数据集地址\n",
    "\n",
    "我们的训练数据集作业地址, 由被挂载的作业训练容器，对应的日志会以arguments (--train /ml/input/data/train/train.csv)形式传递给到作业中\n",
    "\n",
    "- job_output_path_uri: 作业输出地址\n",
    "\n",
    "作业的输出OSS地址，会被挂载到作业容器的`/ml/output`目录下，作业输出的模型，会被持久化到OSS的相应路径下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果本地没有SKLearn，Pandas的包，请执行以下的命令安装.\n",
    "# !{sys.executable} -m pip install scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oss://lq-pai-test-1.oss-cn-hangzhou.aliyuncs.com/custom-job-example/train-data/train.csv\n",
      "oss://lq-pai-test-1.oss-cn-hangzhou.aliyuncs.com/custom-job-example/output/\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "df = pd.DataFrame(\n",
    "    data=np.c_[iris[\"data\"], iris[\"target\"]],\n",
    "    columns=iris[\"feature_names\"] + [\"target\"],\n",
    ")\n",
    "df.to_csv(\"train.csv\", sep=\",\", index=False)\n",
    "\n",
    "oss_bucket = sess.oss_bucket  # type: oss2.Bucket\n",
    "\n",
    "oss_bucket.put_object_from_file(\n",
    "    \"custom-job-example/train-data/train.csv\", filename=\"train.csv\"\n",
    ")\n",
    "\n",
    "job_train_data_uri = (\n",
    "    \"oss://{bucket_name}.{endpoint}/custom-job-example/train-data/train.csv\".format(\n",
    "        bucket_name=oss_bucket.bucket_name,\n",
    "        endpoint=oss_bucket.endpoint.strip(\"https://\"),\n",
    "    )\n",
    ")\n",
    "print(job_train_data_uri)\n",
    "\n",
    "job_output_path_uri = (\n",
    "    \"oss://{bucket_name}.{endpoint}/custom-job-example/output/\".format(\n",
    "        bucket_name=oss_bucket.bucket_name,\n",
    "        endpoint=oss_bucket.endpoint.strip(\"https://\"),\n",
    "    )\n",
    ")\n",
    "print(job_output_path_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 准备训练的脚本\n",
    "\n",
    "我们这里使用XGBoost进行训练，以下的脚本，使用上述准备的数据集进行训练，测试。\n",
    "\n",
    "- 作业定义的参数会以arguments的方式，拉起用户指定的训练脚本。\n",
    "\n",
    "- 在OSS上的数据会被挂载到容器上，并且以arguments的方式，将数据文件的挂载地址传递给到训练脚本。\n",
    "\n",
    "- 指定的作业输出的OSS路径，会被挂载到 `/ml/output` 目录下。\n",
    "训练脚本将模型，以及结果metric写出到相应的本地目录下，即可保存作业的训练产出到OSS。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "source_code_dir = \"source_scripts\"\n",
    "!mkdir -p $source_code_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting source_scripts/xgb_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $source_code_dir/xgb_train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "TRAINING_BASE_DIR = \"/ml/\"\n",
    "TRAINING_CODE_DIR = os.path.join(TRAINING_BASE_DIR, \"code/\")\n",
    "TRAINING_OUTPUT_MODEL_DIR = os.path.join(TRAINING_BASE_DIR, \"output/model/\")\n",
    "TRAINING_OUTPUT_PARAMETER_DIR = os.path.join(TRAINING_BASE_DIR, \"output/output_parameters/\")\n",
    "\n",
    "\n",
    "def load_train_test(data_path):\n",
    "    df = pd.read_csv(data_path, sep=\",\")\n",
    "    train, test = train_test_split(df, test_size=0.3)\n",
    "    train_y = train[\"target\"]\n",
    "    train_x = train.drop([\"target\"], axis=1)\n",
    "    test_y = test[\"target\"]\n",
    "    test_x = test.drop([\"target\"], axis=1)\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"XGBoost train example\")\n",
    "    # 用户指定的任务参数\n",
    "    parser.add_argument(\n",
    "        \"--n_estimators\", type=int, default=500, help=\"The number of base model.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--objective\", type=str, default=\"multi:softmax\", choices=[\"multi:softmax\", \"multi:softprob\"],\n",
    "        help=\"Objective function used by XGBoost, supported objective: {'multi:softmax', 'multi:softprob'}\", )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--max_depth\", type=int, default=3, help=\"The maximum depth of the tree.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--eta\", type=float, default=0.2, help=\"Step size shrinkage used in update to prevents overfitting.\",\n",
    "    )\n",
    "\n",
    "    # 作业数据的数据，也通过arguments的方式传递给到训练脚本.\n",
    "    parser.add_argument(\n",
    "        \"--train_data\", type=str, help=\"Input train data path.\"\n",
    "    )\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "\n",
    "    # 读取传入到容器内的数据\n",
    "    train_x, train_y, test_x, test_y = load_train_test(args.train_data)\n",
    "\n",
    "    # 这里使用XGBoost的SKLearn API进行作业训练.\n",
    "    clf = XGBClassifier(max_depth=args.max_depth,\n",
    "                        eta=args.eta,\n",
    "                        n_estimators=args.n_estimators,\n",
    "                        objective=args.objective)\n",
    "    clf.fit(train_x, train_y)\n",
    "    y_pred = clf.predict(test_x)\n",
    "    accuracy = metrics.accuracy_score(test_y, y_pred)\n",
    "\n",
    "    # 写出作业在测试集上的精度到 /ml/output/output_parameters/test-accuracy 文件\n",
    "    print(\"Output model accuracy=%s\" % accuracy)\n",
    "    os.makedirs(TRAINING_OUTPUT_PARAMETER_DIR, exist_ok=True)\n",
    "    with open(os.path.join(TRAINING_OUTPUT_PARAMETER_DIR, \"test-accuracy\"), \"w\") as f:\n",
    "        f.write(str(accuracy))\n",
    "\n",
    "    # 写出作业产出模型到 /ml/output/model/\n",
    "    os.makedirs(TRAINING_OUTPUT_MODEL_DIR, exist_ok=True)\n",
    "    clf.save_model(f\"{TRAINING_OUTPUT_MODEL_DIR}xgb_model\")\n",
    "    print(f\"Save model succeed: model_path={TRAINING_OUTPUT_MODEL_DIR}xgb_model\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 启动作业进行训练\n",
    "\n",
    "我们使用CustomJob拉起对应的作业\n",
    "\n",
    "- CustomJob将相关的代码上传到OSS\n",
    "对应的source_code目录的文件会被打包上传到OSS中，然后传递到作业容器目录: `/ml/code`。\n",
    "\n",
    "- 将指定数据，以及代码准备到训练作业容器中，通过arguments传递训练的数据，以及参数给到作业脚本，启动作业。\n",
    "\n",
    "以下的作业脚本的作业脚本，启动命令如下。\n",
    "\n",
    "```shell\n",
    "\n",
    "python xgb_train.py \\\n",
    "--n_estimators 500 \\\n",
    "--objective multi:softmax \\\n",
    "--max_depth 5 \\\n",
    "--train_data /ml/input/data/train_data/train.csv\n",
    "\n",
    "```\n",
    "\n",
    "作业的输入数据默认挂载到 `/ml/input/data/{input_name}/`目录下，传入的输出路径(`output_path`)会挂载到`/ml/output`路径下。\n",
    "默认的文件目录结构如下:\n",
    "\n",
    "```shell\n",
    "/ml\n",
    "|-- input\t\t\t\t\t\t\t\t\t\t// 作业输入path\n",
    "|   `-- data\t\t\t\t\t\t\t\t    // 数据数据所在目录，每一个子文件夹表示一个具体输入\n",
    "|       |-- test\n",
    "|       `-- train_data\n",
    "`-- output\t\t\t\t\t\t\t\t\t  // 输出的path, 作业指定的OutputPath（OSS）会被挂载到这个目录下.\n",
    "    |-- model\n",
    "    `-- output_parameters\n",
    "        `-- test_accurary.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pai.job import CustomJob\n",
    "\n",
    "from pai.operator.types import (\n",
    "    PipelineParameter,\n",
    "    PipelineArtifact,\n",
    "    ArtifactMetadataUtils,\n",
    ")\n",
    "\n",
    "# 这里我们使用XGBoost的社区镜像运行脚本，相应的镜像中已经预安装了xgboost, pandas等相关的Python库。\n",
    "image_uri = \"registry.{}.aliyuncs.com/pai-dlc/xgboost-training:1.6.0-cpu-py36-ubuntu18.04\".format(\n",
    "    sess.region_id\n",
    ")\n",
    "\n",
    "job = CustomJob(\n",
    "    entry_point=\"xgb_train.py\",\n",
    "    # 训练作业使用的代码文件夹\n",
    "    source_code=source_code_dir,\n",
    "    # 训练作业使用镜像\n",
    "    image_uri=image_uri,\n",
    "    # 训练作业参数，通过arguments传递给到脚本.\n",
    "    parameters={\n",
    "        \"n_estimators\": 500,\n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"max_depth\": 5,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 用户可以通过LocalRun的方式，在本地调试对应的脚本。\n",
    "# job.local_run 通过运行一个Docker container的方式，模拟的作业的执行。\n",
    "# 相应的传入的数据，会被mount到Docker container中，然后以arguments的方式传递给到脚本。\n",
    "\n",
    "# job.local_run(\n",
    "#     inputs={\n",
    "#         \"train_data\": \"./train.csv\",\n",
    "#     },\n",
    "#     output_path=\"./output/\"\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# 提交任务\n",
    "job_id = job.run(\n",
    "    name=\"custom-job-example\",\n",
    "    # 作业的执行配置\n",
    "    job_config=JobConfig.create(worker_count=1, worker_instance_type=\"ecs.c6.large\"),\n",
    "    inputs={\n",
    "        \"train_data\": job_train_data_uri,\n",
    "    },\n",
    "    output_path=job_output_path_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "训练产出的模型可以通过OSS client下载到本地应用，也可以直接在PAI的控制台上，使用PAI-EAS进行部署。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oss://lq-pai-test-1.oss-cn-hangzhou.aliyuncs.com/custom-job-example/output/model/xgb_model\n",
      "total 960\r\n",
      "144 -rw-r--r--  1 liangquan  staff   71391 Jun 24 14:26 custom_job.ipynb\r\n",
      " 40 -rw-r--r--  1 liangquan  staff   19615 Jun 23 13:54 custom_job_inner.ipynb\r\n",
      "  0 drwxr-xr-x  4 liangquan  staff     128 Jun  9 15:18 \u001B[1m\u001B[36msource_scripts\u001B[m\u001B[m\r\n",
      "  8 -rw-r--r--  1 liangquan  staff    3077 Jun 24 14:25 train.csv\r\n",
      "768 -rw-r--r--  1 liangquan  staff  349620 Jun 24 14:26 xgb_model\r\n"
     ]
    }
   ],
   "source": [
    "from pai.common.oss_utils import parse_oss_url\n",
    "\n",
    "model_url = job_output_path_uri + \"model/xgb_model\"\n",
    "print(model_url)\n",
    "object_key = parse_oss_url(model_url).object_key\n",
    "oss_bucket.get_object_to_file(object_key, \"xgb_model\")\n",
    "!ls -ls ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##（可选）自定义作业任务作为组件保存\n",
    "\n",
    "以上构建的`CustomJob`实例可以作为一个Workflow的组件复用。\n",
    "用户可以在这个组件上使用不同的参数，数据集，或是计算资源配置运行相应的脚本作业。同时这个组件也可以作为工作流中的节点，构建一个Workflow。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegisteredComponent:Id=pipeline-3q55gzd66ne4tv14nm,Identifier=xgb-example,Provider=1157703270994901,Version=v-1656051987\n",
      "InputsSpec:\n",
      "\tPipelineParameter:{Name:job_config, Kind:inputs, Required:True, Value:None, Desc:PAI-DLC job config, including worker spec, resource spec, etc.}\n",
      "\tPipelineParameter:{Name:output_path, Kind:inputs, Required:False, Value:, Desc:Job output path, could be OSS url or NAS url.}\n",
      "\tPipelineParameter:{Name:n_estimators, Kind:inputs, Required:False, Value:500, Desc:None}\n",
      "\tPipelineParameter:{Name:objective, Kind:inputs, Required:False, Value:multi:softmax, Desc:None}\n",
      "\tPipelineParameter:{Name:max_depth, Kind:inputs, Required:False, Value:5, Desc:None}\n",
      "\tPipelineArtifact:{Name:train_data, Kind:inputs, Required:False, Value:None, Desc:None}\n"
     ]
    }
   ],
   "source": [
    "from pai.operator import CustomJobOperator, RegisteredComponent\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "# 构建一个Workflow的组件，由Workflow服务来提交对应的作业\n",
    "op: CustomJobOperator = job.as_component(\n",
    "    inputs=[\n",
    "        PipelineArtifact(\n",
    "            \"train_data\",\n",
    "            metadata=ArtifactMetadataUtils.oss_dataset(),\n",
    "        )\n",
    "    ],\n",
    "    outputs=[PipelineParameter(\"test-accuracy\")],\n",
    ")\n",
    "\n",
    "\n",
    "# 组件可以注册保存到PAI的服务后端\n",
    "\n",
    "version = \"v-%s\" % int(time.time())\n",
    "op.save(identifier=\"xgb-example\", version=version)\n",
    "# 从后端获取保存的组件\n",
    "registered_op = RegisteredComponent.get_by_identifier(\n",
    "    identifier=\"xgb-example\", version=version\n",
    ")\n",
    "\n",
    "print(registered_op)\n",
    "print(registered_op.inputs)\n",
    "\n",
    "\n",
    "# 使用保存的组件运行拉起对应的作业\n",
    "registered_op.run(\n",
    "    job_name=\"Hello\",\n",
    "    arguments={\n",
    "        \"job_config\": JobConfig.create(\n",
    "            worker_count=1, worker_instance_type=\"ecs.c6.large\"\n",
    "        ).to_dict(),\n",
    "        \"output_path\": job_output_path_uri,\n",
    "        \"train_data\": job_train_data_uri,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##（可选）构建Workflow\n",
    "\n",
    "对应的作业任务组件能够用于构建一个Workflow DAG，以下的样例中，使用了上述的脚本构建了包含条件分支判断的Workflow。\n",
    "\n",
    "当节点`train-step-1`在测试集上的精度低于0.90时运行`train-step-2`, 而在精度高于0.90时运行`train-step-3`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create pipeline run success (run_id: flow-x37wsw8x0zhqw6ai8j), please visit the link below to view the run detail.\n",
      "https://pai.console.aliyun.com/console?regionId=cn-hangzhou#/studio/task/detail/flow-x37wsw8x0zhqw6ai8j\n",
      "Wait for run workflow init\n",
      "Add Node Logger: example-custom-job-workflow, node-cqlpd6334w8yiciuvo\n",
      "Add Node Logger: example-custom-job-workflow.train-step-1, node-m0s8i9wlz1lfa166s7\n",
      "Add Node Logger: example-custom-job-workflow.train-step-2, node-0doox0zekams5kriza\n",
      "Add Node Logger: example-custom-job-workflow.train-step-3, node-2svkf5cmzaniitl1p1\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:53.831068322+08:00 stderr F /usr/local/lib/python3.6/dist-packages/aliyunsdkcore/auth/algorithm/sha_hmac256.py:20: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:53.831313102+08:00 stderr F   from cryptography.hazmat.backends import default_backend\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.238933401+08:00 stderr F 2022/06/24 06:26:54 INFO: pai_running_utils: 0.4.0.dev27\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.239651451+08:00 stderr F 2022/06/24 06:26:54 INFO: Env PAI_SERVICE_ENV=\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.240492005+08:00 stderr F 2022/06/24 06:26:54 INFO: Env PAI_PIPELINE_VERSION=\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.240758757+08:00 stderr F 2022/06/24 06:26:54 INFO: Env PAI_PIPELINE_METADATA_IDENTIFIER=\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.241594923+08:00 stderr F 2022/06/24 06:26:54 INFO: Env PAI_PIPELINE_METADATA_VERSION=\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.242407386+08:00 stderr F 2022/06/24 06:26:54 INFO: Env PAI_PIPELINE_METADATA_PROVIDER=\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.242775797+08:00 stderr F 2022/06/24 06:26:54 INFO: Env PAI_PARENT_USER_ID=\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.243595006+08:00 stderr F 2022/06/24 06:26:54 INFO: Env PAI_RUN_ID=flow-x37wsw8x0zhqw6ai8j\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.243913787+08:00 stderr F 2022/06/24 06:26:54 INFO: Env PAI_NODE_ID=node-m0s8i9wlz1lfa166s7\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.244793085+08:00 stderr F 2022/06/24 06:26:54 INFO: Env PAI_AI_WORKSPACE_ID=\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.245614264+08:00 stderr F 2022/06/24 06:26:54 INFO: Env PAI_INPUTS_PARAMETERS=\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.246448233+08:00 stderr F 2022/06/24 06:26:54 INFO: Env PAI_MANIFEST_SPEC_INPUTS=\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.247229344+08:00 stderr F 2022/06/24 06:26:54 INFO: Env PAI_MANIFEST_SPEC_OUTPUTS=\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.248126127+08:00 stderr F 2022/06/24 06:26:54 INFO: Env PAI_BASE_DIR=\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.248257925+08:00 stderr F 2022/06/24 06:26:54 INFO: Env PAI_REGION_ID=\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.248359003+08:00 stderr F 2022/06/24 06:26:54 INFO: Env PAI_ACCESS_SECRET=AccessKeySecret\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.248453818+08:00 stderr F 2022/06/24 06:26:54 INFO: Env PAI_ACCESS_KEY=TMP.xxxxxxxxxxhCgA\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.250586683+08:00 stderr F 2022/06/24 06:26:54 INFO: Initialize context from file.\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.251818779+08:00 stderr F 2022/06/24 06:26:54 INFO: input parameters: {'n_estimators': 500, 'output_path': 'oss://lq-pai-test-1.oss-cn-hangzhou.aliyuncs.com/custom-job-example/output/train-step-1/', 'job_config': {'JobSpecs': [{'EcsSpec': 'ecs.c6.large', 'ImageUri': None, 'PodCount': 1, 'Type': 'Worker'}]}, 'max_depth': 5, 'objective': 'multi:softmax'}\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.25216587+08:00 stderr F 2022/06/24 06:26:54 INFO: Input artifacts: name=train_data, type=OssLocationArtifact, raw_value={\"location\": {\"bucket\": \"lq-pai-test-1\", \"endpoint\": \"oss-cn-hangzhou.aliyuncs.com\", \"key\": \"custom-job-example/train-data/train.csv\"}}\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.252364417+08:00 stderr F 2022/06/24 06:26:54 INFO: Initialize context from file.\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.255828367+08:00 stderr F 2022/06/24 06:26:54 INFO: Initialize context from file.\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.256259697+08:00 stderr F 2022/06/24 06:26:54 INFO: CustomJobSpec:\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.256539926+08:00 stderr F 2022/06/24 06:26:54 INFO: Job spec image_uri=registry.cn-hangzhou.aliyuncs.com/pai-dlc/xgboost-training:1.6.0-cpu-py36-ubuntu18.04 job_config={'JobSpecs': [{'EcsSpec': 'ecs.c6.large', 'ImageUri': None, 'PodCount': 1, 'Type': 'Worker'}]} job_type=TFJob\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.258260878+08:00 stderr F 2022/06/24 06:26:54 INFO: Job entry_point=xgb_train.py source_files=oss://lq-pai-test-1.oss-cn-hangzhou.aliyuncs.com/pai/custom_job/20220624-14:26:27/source.tar.gz command=None\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.258629249+08:00 stderr F 2022/06/24 06:26:54 INFO: Job output_path=oss://lq-pai-test-1.oss-cn-hangzhou.aliyuncs.com/custom-job-example/output/train-step-1/ oss_role_arn=None oss_aliyun_uid=None\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.258870059+08:00 stderr F 2022/06/24 06:26:54 INFO: Initialize context from file.\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.260126036+08:00 stderr F 2022/06/24 06:26:54 INFO: Initialize context from file.\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.263360633+08:00 stderr F 2022/06/24 06:26:54 INFO: Initialize context from file.\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.416168912+08:00 stderr F 2022/06/24 06:26:54 INFO: create dataset using workspace service api.\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.416333819+08:00 stderr F 2022/06/24 06:26:54 INFO: Initialize context from file.\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.488868463+08:00 stderr F 2022/06/24 06:26:54 INFO: Prepare dataset: input_name=train_data, dataset_id=d-494om0ljl0f1y0xn3k, path=oss://lq-pai-test-1/custom-job-example/train-data/\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.499128368+08:00 stderr F 2022/06/24 06:26:54 INFO: Initialize context from file.\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.499222073+08:00 stderr F 2022/06/24 06:26:54 INFO: create dataset using workspace service api.\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.499291234+08:00 stderr F 2022/06/24 06:26:54 INFO: Initialize context from file.\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.566598616+08:00 stderr F 2022/06/24 06:26:54 INFO: Prepare output dataset: dataset_id=d-po0xl428hg9rqwf6s8, path=oss://lq-pai-test-1.oss-cn-hangzhou.aliyuncs.com/custom-job-example/output/train-step-1/\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.566691792+08:00 stderr F 2022/06/24 06:26:54 INFO: Initialize context from file.\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.56887002+08:00 stderr F 2022/06/24 06:26:54 INFO: create dataset using workspace service api.\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.569737731+08:00 stderr F 2022/06/24 06:26:54 INFO: Initialize context from file.\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.636456927+08:00 stdout F CreateJobRequest: %s {'DataSources': [{'DataSourceId': 'd-494om0ljl0f1y0xn3k'}, {'DataSourceId': 'd-po0xl428hg9rqwf6s8'}, {'DataSourceId': 'd-o9wcpqwjyu5kjq5wr9'}], 'DisplayName': 'custom-job-None', 'JobSpecs': [{'EcsSpec': 'ecs.c6.large', 'Image': 'registry.cn-hangzhou.aliyuncs.com/pai-dlc/xgboost-training:1.6.0-cpu-py36-ubuntu18.04', 'PodCount': 1, 'Type': 'Worker'}], 'JobType': 'TFJob', 'UserCommand': 'mkdir -p /ml/code && cd /ml/code  && tar -xvzf /ml/mount/code/source.tar.gz -C /ml/code  && python xgb_train.py --n_estimators 500 --max_depth 5 --objective multi:softmax --train_data /ml/input/data/train_data/train.csv', 'WorkspaceId': '28293'}\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.636389508+08:00 stderr F 2022/06/24 06:26:54 INFO: Prepare source code as dataset: dataset_id=d-o9wcpqwjyu5kjq5wr9\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.636667749+08:00 stderr F 2022/06/24 06:26:54 INFO: CreateJobRequest: {'DataSources': [{'DataSourceId': 'd-494om0ljl0f1y0xn3k'}, {'DataSourceId': 'd-po0xl428hg9rqwf6s8'}, {'DataSourceId': 'd-o9wcpqwjyu5kjq5wr9'}], 'DisplayName': 'custom-job-None', 'JobSpecs': [{'EcsSpec': 'ecs.c6.large', 'Image': 'registry.cn-hangzhou.aliyuncs.com/pai-dlc/xgboost-training:1.6.0-cpu-py36-ubuntu18.04', 'PodCount': 1, 'Type': 'Worker'}], 'JobType': 'TFJob', 'UserCommand': 'mkdir -p /ml/code && cd /ml/code  && tar -xvzf /ml/mount/code/source.tar.gz -C /ml/code  && python xgb_train.py --n_estimators 500 --max_depth 5 --objective multi:softmax --train_data /ml/input/data/train_data/train.csv', 'WorkspaceId': '28293'}\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:54.636746981+08:00 stderr F 2022/06/24 06:26:54 INFO: Initialize context from file.\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:55.092323209+08:00 stderr F 2022/06/24 06:26:55 INFO: Initialize context from file.\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:55.097301264+08:00 stderr F 2022/06/24 06:26:55 INFO: Job url: https://pai.console.aliyun.com/?regionId=cn-hangzhou&workspaceId=28293#/job/detail?jobId=dlc1cbv9j698itrx\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:55.09947649+08:00 stderr F 2022/06/24 06:26:55 INFO: DlcJobUtils persist job id: dlc1cbv9j698itrx\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:55.100295678+08:00 stderr F 2022/06/24 06:26:55 INFO: Initialize context from file.\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:26:55.446709448+08:00 stderr F 2022/06/24 06:26:55 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:27:05.809575628+08:00 stderr F 2022/06/24 06:27:05 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:27:16.270774442+08:00 stderr F 2022/06/24 06:27:16 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:27:26.636852782+08:00 stderr F 2022/06/24 06:27:26 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:27:37.006595962+08:00 stderr F 2022/06/24 06:27:37 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:27:47.394225662+08:00 stderr F 2022/06/24 06:27:47 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:27:57.733200366+08:00 stderr F 2022/06/24 06:27:57 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:28:08.085689565+08:00 stderr F 2022/06/24 06:28:08 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:28:18.432462939+08:00 stderr F 2022/06/24 06:28:18 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:28:28.778034917+08:00 stderr F 2022/06/24 06:28:28 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:28:39.12965191+08:00 stderr F 2022/06/24 06:28:39 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:28:49.488992447+08:00 stderr F 2022/06/24 06:28:49 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:28:59.840182671+08:00 stderr F 2022/06/24 06:28:59 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:29:10.176050391+08:00 stderr F 2022/06/24 06:29:10 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:29:20.504142313+08:00 stderr F 2022/06/24 06:29:20 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:29:30.849690376+08:00 stderr F 2022/06/24 06:29:30 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:29:41.218297291+08:00 stderr F 2022/06/24 06:29:41 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:29:51.566356453+08:00 stderr F 2022/06/24 06:29:51 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:30:02.112420817+08:00 stderr F 2022/06/24 06:30:02 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:30:12.480342952+08:00 stderr F 2022/06/24 06:30:12 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:30:22.83701445+08:00 stderr F 2022/06/24 06:30:22 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:30:33.223778076+08:00 stderr F 2022/06/24 06:30:33 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:30:43.563376445+08:00 stderr F 2022/06/24 06:30:43 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:30:53.919077131+08:00 stderr F 2022/06/24 06:30:53 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:31:04.31200033+08:00 stderr F 2022/06/24 06:31:04 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:31:14.704809929+08:00 stderr F 2022/06/24 06:31:14 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:31:25.048791524+08:00 stderr F 2022/06/24 06:31:25 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:31:35.423456975+08:00 stderr F 2022/06/24 06:31:35 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:31:45.80549952+08:00 stderr F 2022/06/24 06:31:45 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:31:56.178646942+08:00 stderr F 2022/06/24 06:31:56 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:32:06.523963357+08:00 stderr F 2022/06/24 06:32:06 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:32:16.859208409+08:00 stderr F 2022/06/24 06:32:16 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:32:27.250796923+08:00 stderr F 2022/06/24 06:32:27 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:32:37.622200445+08:00 stderr F 2022/06/24 06:32:37 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:32:47.989493441+08:00 stderr F 2022/06/24 06:32:47 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:32:58.337768081+08:00 stderr F 2022/06/24 06:32:58 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:33:08.704202877+08:00 stderr F 2022/06/24 06:33:08 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:33:19.02937306+08:00 stderr F 2022/06/24 06:33:19 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:33:29.659468136+08:00 stderr F 2022/06/24 06:33:29 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:33:39.99731665+08:00 stderr F 2022/06/24 06:33:39 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:33:50.361857698+08:00 stderr F 2022/06/24 06:33:50 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:34:00.812987648+08:00 stderr F 2022/06/24 06:34:00 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:34:11.176133188+08:00 stderr F 2022/06/24 06:34:11 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:34:21.522954073+08:00 stderr F 2022/06/24 06:34:21 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:34:31.866576184+08:00 stderr F 2022/06/24 06:34:31 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:34:42.239237865+08:00 stderr F 2022/06/24 06:34:42 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:34:52.663372526+08:00 stderr F 2022/06/24 06:34:52 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:35:03.167387208+08:00 stderr F 2022/06/24 06:35:03 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:35:13.499863283+08:00 stderr F 2022/06/24 06:35:13 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:35:23.85406292+08:00 stderr F 2022/06/24 06:35:23 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:35:34.196676905+08:00 stderr F 2022/06/24 06:35:34 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:35:44.536971059+08:00 stderr F 2022/06/24 06:35:44 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:35:54.876834735+08:00 stderr F 2022/06/24 06:35:54 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:36:05.246158599+08:00 stderr F 2022/06/24 06:36:05 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:36:15.63035001+08:00 stderr F 2022/06/24 06:36:15 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:36:26.114976434+08:00 stderr F 2022/06/24 06:36:26 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:36:36.464397921+08:00 stderr F 2022/06/24 06:36:36 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:36:46.802653712+08:00 stderr F 2022/06/24 06:36:46 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:36:57.152304293+08:00 stderr F 2022/06/24 06:36:57 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:37:07.48766729+08:00 stderr F 2022/06/24 06:37:07 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:37:17.849002722+08:00 stderr F 2022/06/24 06:37:17 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:37:28.211165798+08:00 stderr F 2022/06/24 06:37:28 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:37:38.55900742+08:00 stderr F 2022/06/24 06:37:38 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:37:48.92349651+08:00 stderr F 2022/06/24 06:37:48 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:37:59.257884727+08:00 stderr F 2022/06/24 06:37:59 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:38:09.586186439+08:00 stderr F 2022/06/24 06:38:09 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:38:19.929895807+08:00 stderr F 2022/06/24 06:38:19 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:38:30.313182638+08:00 stderr F 2022/06/24 06:38:30 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:38:40.680152848+08:00 stderr F 2022/06/24 06:38:40 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:38:51.047058408+08:00 stderr F 2022/06/24 06:38:51 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:39:01.418211748+08:00 stderr F 2022/06/24 06:39:01 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:39:11.817499541+08:00 stderr F 2022/06/24 06:39:11 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:39:22.187909531+08:00 stderr F 2022/06/24 06:39:22 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:39:32.520744596+08:00 stderr F 2022/06/24 06:39:32 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:39:42.880502726+08:00 stderr F 2022/06/24 06:39:42 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:39:53.234940617+08:00 stderr F 2022/06/24 06:39:53 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:40:03.774582182+08:00 stderr F 2022/06/24 06:40:03 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:40:14.122436826+08:00 stderr F 2022/06/24 06:40:14 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:40:24.481657009+08:00 stderr F 2022/06/24 06:40:24 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:40:34.84827374+08:00 stderr F 2022/06/24 06:40:34 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:40:45.292230557+08:00 stderr F 2022/06/24 06:40:45 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:40:55.631745617+08:00 stderr F 2022/06/24 06:40:55 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:41:05.967071788+08:00 stderr F 2022/06/24 06:41:05 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:41:16.330021029+08:00 stderr F 2022/06/24 06:41:16 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:41:26.686743695+08:00 stderr F 2022/06/24 06:41:26 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:41:37.044984125+08:00 stderr F 2022/06/24 06:41:37 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:41:47.41932688+08:00 stderr F 2022/06/24 06:41:47 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:41:57.784302969+08:00 stderr F 2022/06/24 06:41:57 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:42:08.118112068+08:00 stderr F 2022/06/24 06:42:08 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:42:18.482926278+08:00 stderr F 2022/06/24 06:42:18 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:42:28.871038147+08:00 stderr F 2022/06/24 06:42:28 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:42:39.215274924+08:00 stderr F 2022/06/24 06:42:39 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:42:49.589942202+08:00 stderr F 2022/06/24 06:42:49 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:42:59.949647767+08:00 stderr F 2022/06/24 06:42:59 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:43:10.540980778+08:00 stderr F 2022/06/24 06:43:10 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:43:20.886833964+08:00 stderr F 2022/06/24 06:43:20 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:43:31.253003736+08:00 stderr F 2022/06/24 06:43:31 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:43:41.649965938+08:00 stderr F 2022/06/24 06:43:41 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:43:52.006371797+08:00 stderr F 2022/06/24 06:43:52 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:44:02.393099523+08:00 stderr F 2022/06/24 06:44:02 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:44:12.74646149+08:00 stderr F 2022/06/24 06:44:12 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:44:23.096923838+08:00 stderr F 2022/06/24 06:44:23 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:44:33.478914607+08:00 stderr F 2022/06/24 06:44:33 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:44:43.842509259+08:00 stderr F 2022/06/24 06:44:43 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:44:54.177316997+08:00 stderr F 2022/06/24 06:44:54 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:45:04.523820179+08:00 stderr F 2022/06/24 06:45:04 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:45:14.870312855+08:00 stderr F 2022/06/24 06:45:14 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:45:25.227449122+08:00 stderr F 2022/06/24 06:45:25 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:45:35.575552001+08:00 stderr F 2022/06/24 06:45:35 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:45:46.038292841+08:00 stderr F 2022/06/24 06:45:46 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:45:56.417464082+08:00 stderr F 2022/06/24 06:45:56 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:46:06.792282702+08:00 stderr F 2022/06/24 06:46:06 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:46:17.160499751+08:00 stderr F 2022/06/24 06:46:17 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:46:27.50649698+08:00 stderr F 2022/06/24 06:46:27 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:46:37.857288738+08:00 stderr F 2022/06/24 06:46:37 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:46:48.225187309+08:00 stderr F 2022/06/24 06:46:48 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:46:58.595196379+08:00 stderr F 2022/06/24 06:46:58 INFO: job status: Creating\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:47:08.936029614+08:00 stderr F 2022/06/24 06:47:08 INFO: job status: Failed\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:47:08.936412929+08:00 stderr F 2022/06/24 06:47:08 ERROR: CustomJob execution failed: job_id=dlc1cbv9j698itrx, status=Failed\n",
      "example-custom-job-workflow.train-step-1: 2022-06-24T14:47:08.937178201+08:00 stderr F 2022/06/24 06:47:08 INFO: CustomJobExecutor delete created tmp dataset ids=['d-494om0ljl0f1y0xn3k', 'd-po0xl428hg9rqwf6s8', 'd-o9wcpqwjyu5kjq5wr9']\n"
     ]
    },
    {
     "ename": "PAIException",
     "evalue": "PipelineRun failed: run_id=flow-x37wsw8x0zhqw6ai8j, run_status_info={'example-custom-job-workflow': {'name': 'tmp-kpx4d6yqtektr59z-x37wsw8x0zhqw6ai8j', 'nodeId': 'node-cqlpd6334w8yiciuvo', 'status': 'Failed', 'startedAt': '2022-06-24T06:26:36.000Z', 'finishedAt': '2022-06-24T06:47:49.000Z'}, 'example-custom-job-workflow.train-step-1': {'name': 'train-step-1', 'nodeId': 'node-m0s8i9wlz1lfa166s7', 'status': 'Failed', 'startedAt': '2022-06-24T06:26:36.000Z', 'finishedAt': '2022-06-24T06:47:39.000Z'}, 'example-custom-job-workflow.train-step-2': {'name': 'train-step-2', 'nodeId': 'node-0doox0zekams5kriza', 'status': 'Skipped', 'startedAt': '2022-06-24T06:47:49.000Z', 'finishedAt': '2022-06-24T06:47:49.000Z'}, 'example-custom-job-workflow.train-step-3': {'name': 'train-step-3', 'nodeId': 'node-2svkf5cmzaniitl1p1', 'status': 'Skipped', 'startedAt': '2022-06-24T06:47:49.000Z', 'finishedAt': '2022-06-24T06:47:49.000Z'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPAIException\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-10-54ffb561bff2>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     45\u001B[0m )\n\u001B[1;32m     46\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 47\u001B[0;31m \u001B[0mp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"example-custom-job-workflow\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     48\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/code/pypai/pai/operator/_base.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, job_name, wait, arguments, show_outputs, **kwargs)\u001B[0m\n\u001B[1;32m    163\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mwait\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    164\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mrun_instance\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 165\u001B[0;31m         \u001B[0mrun_instance\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait_for_completion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mshow_outputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mshow_outputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    166\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mrun_instance\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    167\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/code/pypai/pai/pipeline/run.py\u001B[0m in \u001B[0;36mwait_for_completion\u001B[0;34m(self, show_outputs, timeout)\u001B[0m\n\u001B[1;32m    327\u001B[0m             \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdebug\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"catch expections, %s\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m             \u001B[0mrun_logger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstop_tail\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 329\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    330\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    331\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mlog_runner\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mlog_runners\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/code/pypai/pai/pipeline/run.py\u001B[0m in \u001B[0;36mwait_for_completion\u001B[0;34m(self, show_outputs, timeout)\u001B[0m\n\u001B[1;32m    306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    307\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mroot_node_status\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mPipelineRunStatus\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mFailed\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 308\u001B[0;31m                     raise PAIException(\n\u001B[0m\u001B[1;32m    309\u001B[0m                         \"PipelineRun failed: run_id={}, run_status_info={}\".format(\n\u001B[1;32m    310\u001B[0m                             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcurr_status_infos\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mPAIException\u001B[0m: PipelineRun failed: run_id=flow-x37wsw8x0zhqw6ai8j, run_status_info={'example-custom-job-workflow': {'name': 'tmp-kpx4d6yqtektr59z-x37wsw8x0zhqw6ai8j', 'nodeId': 'node-cqlpd6334w8yiciuvo', 'status': 'Failed', 'startedAt': '2022-06-24T06:26:36.000Z', 'finishedAt': '2022-06-24T06:47:49.000Z'}, 'example-custom-job-workflow.train-step-1': {'name': 'train-step-1', 'nodeId': 'node-m0s8i9wlz1lfa166s7', 'status': 'Failed', 'startedAt': '2022-06-24T06:26:36.000Z', 'finishedAt': '2022-06-24T06:47:39.000Z'}, 'example-custom-job-workflow.train-step-2': {'name': 'train-step-2', 'nodeId': 'node-0doox0zekams5kriza', 'status': 'Skipped', 'startedAt': '2022-06-24T06:47:49.000Z', 'finishedAt': '2022-06-24T06:47:49.000Z'}, 'example-custom-job-workflow.train-step-3': {'name': 'train-step-3', 'nodeId': 'node-2svkf5cmzaniitl1p1', 'status': 'Skipped', 'startedAt': '2022-06-24T06:47:49.000Z', 'finishedAt': '2022-06-24T06:47:49.000Z'}}"
     ]
    }
   ],
   "source": [
    "from pai.pipeline import Pipeline\n",
    "\n",
    "step1 = op.as_step(\n",
    "    name=\"train-step-1\",\n",
    "    inputs={\n",
    "        \"job_config\": JobConfig.create(\n",
    "            worker_count=1, worker_instance_type=\"ecs.c6.large\"\n",
    "        ).to_dict(),\n",
    "        \"output_path\": job_output_path_uri + \"train-step-1/\",\n",
    "        \"train_data\": job_train_data_uri,\n",
    "        \"n_estimators\": 500,\n",
    "    },\n",
    ")\n",
    "\n",
    "step2 = op.as_condition_step(\n",
    "    name=\"train-step-2\",\n",
    "    condition=step1.outputs[\"test-accuracy\"] <= 0.90,\n",
    "    inputs={\n",
    "        \"job_config\": JobConfig.create(\n",
    "            worker_count=1, worker_instance_type=\"ecs.c6.large\"\n",
    "        ).to_dict(),\n",
    "        \"output_path\": job_output_path_uri + \"train-step-2/\",\n",
    "        \"train_data\": job_train_data_uri,\n",
    "        \"n_estimators\": 500,\n",
    "    },\n",
    "    depends=[step1],\n",
    ")\n",
    "\n",
    "step3 = op.as_condition_step(\n",
    "    name=\"train-step-3\",\n",
    "    condition=step1.outputs[\"test-accuracy\"] > 0.90,\n",
    "    inputs={\n",
    "        \"job_config\": JobConfig.create(\n",
    "            worker_count=1, worker_instance_type=\"ecs.c6.large\"\n",
    "        ).to_dict(),\n",
    "        \"output_path\": job_output_path_uri + \"train-step-3/\",\n",
    "        \"train_data\": job_train_data_uri,\n",
    "        \"n_estimators\": 1000,\n",
    "    },\n",
    "    depends=[step1],\n",
    ")\n",
    "\n",
    "p = Pipeline(\n",
    "    steps=[step2, step3],\n",
    ")\n",
    "\n",
    "p.run(\"example-custom-job-workflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
