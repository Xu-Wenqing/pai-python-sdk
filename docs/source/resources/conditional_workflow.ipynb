{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 在工作流中使用条件执行\n",
    "\n",
    "PAI的Workflow支持在条件执行，从而支持更为灵活的Workflow执行逻辑。以下的示例中，我们展示了如何通过SDK构建带条件执行的工作流。\n",
    "\n",
    "\n",
    "## 准备工作\n",
    "\n",
    "请首先安装PAI SDK，以支持运行以下的示例代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\r\n",
      "Collecting alipai==0.3.4a1\r\n",
      "  Downloading https://pai-sdk.oss-cn-shanghai.aliyuncs.com/alipai/dist/alipai-0.3.4a1-py2.py3-none-any.whl (162 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 162 kB 2.2 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: six>=1.15.0 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages (from alipai==0.3.4a1) (1.16.0)\r\n",
      "Requirement already satisfied: graphviz<0.17 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/graphviz-0.16-py3.6.egg (from alipai==0.3.4a1) (0.16)\r\n",
      "Requirement already satisfied: docker>=4.4.0 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/docker-5.0.3-py3.6.egg (from alipai==0.3.4a1) (5.0.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages (from alipai==0.3.4a1) (5.3.1)\r\n",
      "Requirement already satisfied: pyodps>=0.11.0 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/pyodps-0.11.0-py3.6-macosx-10.9-x86_64.egg (from alipai==0.3.4a1) (0.11.0)\r\n",
      "Requirement already satisfied: alibabacloud-tea-openapi-py2<1.0.0,>=0.0.2 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/alibabacloud_tea_openapi_py2-0.1.3-py3.6.egg (from alipai==0.3.4a1) (0.1.3)\r\n",
      "Requirement already satisfied: aliyun-python-sdk-core==2.13.25 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/aliyun_python_sdk_core-2.13.25-py3.6.egg (from alipai==0.3.4a1) (2.13.25)\r\n",
      "Requirement already satisfied: alibabacloud-endpoint-util-py2<1.0.0,>=0.0.1 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/alibabacloud_endpoint_util_py2-0.0.2-py3.6.egg (from alipai==0.3.4a1) (0.0.2)\r\n",
      "Collecting importlib-metadata<=2.1.0,>=2.0.0\r\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/9b/f1/1e6b4b2af60ab7999c8d5317ab321ebe9ad3820cbe2033ee99be77936afd/importlib_metadata-2.1.0-py2.py3-none-any.whl (10 kB)\r\n",
      "Collecting numpy<=1.18.0,>=1.16.0\r\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/7c/cd/5243645399c09bb5081e8d2847583f7a6b7cca55eb096a880eda0b602d4d/numpy-1.18.0-cp36-cp36m-macosx_10_9_x86_64.whl (15.2 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 15.2 MB 122 kB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: alibabacloud-tea-util-py2<1.0.0,>=0.0.2 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/alibabacloud_tea_util_py2-0.0.5-py3.6.egg (from alipai==0.3.4a1) (0.0.5)\r\n",
      "Requirement already satisfied: oss2>=2.8.0 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/oss2-2.15.0-py3.6.egg (from alipai==0.3.4a1) (2.15.0)\r\n",
      "Requirement already satisfied: alibabacloud-openapi-util-py2<1.0.0,>=0.0.4 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/alibabacloud_openapi_util_py2-0.0.8-py3.6.egg (from alipai==0.3.4a1) (0.0.8)\r\n",
      "Requirement already satisfied: aliyun-python-sdk-sts>=3.0.2 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/aliyun_python_sdk_sts-3.1.0-py3.6.egg (from alipai==0.3.4a1) (3.1.0)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.9.3 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/jmespath-0.10.0-py3.6.egg (from aliyun-python-sdk-core==2.13.25->alipai==0.3.4a1) (0.10.0)\r\n",
      "Requirement already satisfied: pycryptodome>=3.4.7 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/pycryptodome-3.14.1-py3.6-macosx-10.9-x86_64.egg (from aliyun-python-sdk-core==2.13.25->alipai==0.3.4a1) (3.14.1)\r\n",
      "Requirement already satisfied: alibabacloud-tea-py2 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/alibabacloud_tea_py2-0.0.4-py3.6.egg (from alibabacloud-endpoint-util-py2<1.0.0,>=0.0.1->alipai==0.3.4a1) (0.0.4)\r\n",
      "Requirement already satisfied: cryptography>=2.6.0 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/cryptography-37.0.2-py3.6-macosx-10.9-x86_64.egg (from alibabacloud-openapi-util-py2<1.0.0,>=0.0.4->alipai==0.3.4a1) (37.0.2)\r\n",
      "Requirement already satisfied: alibabacloud_credentials_py2<1.0.0,>=0.0.1 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/alibabacloud_credentials_py2-0.0.3-py3.6.egg (from alibabacloud-tea-openapi-py2<1.0.0,>=0.0.2->alipai==0.3.4a1) (0.0.3)\r\n",
      "Requirement already satisfied: alibabacloud_gateway_spi_py2<1.0.0,>=0.0.1 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/alibabacloud_gateway_spi_py2-0.0.1-py3.6.egg (from alibabacloud-tea-openapi-py2<1.0.0,>=0.0.2->alipai==0.3.4a1) (0.0.1)\r\n",
      "Requirement already satisfied: alibabacloud_tea_xml_py2<1.0.0,>=0.0.2 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/alibabacloud_tea_xml_py2-0.0.2-py3.6.egg (from alibabacloud-tea-openapi-py2<1.0.0,>=0.0.2->alipai==0.3.4a1) (0.0.2)\r\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages (from cryptography>=2.6.0->alibabacloud-openapi-util-py2<1.0.0,>=0.0.4->alipai==0.3.4a1) (1.14.6)\r\n",
      "Requirement already satisfied: pycparser in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages (from cffi>=1.12->cryptography>=2.6.0->alibabacloud-openapi-util-py2<1.0.0,>=0.0.4->alipai==0.3.4a1) (2.21)\r\n",
      "Requirement already satisfied: requests!=2.18.0,>=2.14.2 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/requests-2.27.1-py3.6.egg (from docker>=4.4.0->alipai==0.3.4a1) (2.27.1)\r\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/websocket_client-1.3.2-py3.6.egg (from docker>=4.4.0->alipai==0.3.4a1) (1.3.2)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages (from importlib-metadata<=2.1.0,>=2.0.0->alipai==0.3.4a1) (3.6.0)\r\n",
      "Requirement already satisfied: crcmod>=1.7 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/crcmod-1.7-py3.6-macosx-10.9-x86_64.egg (from oss2>=2.8.0->alipai==0.3.4a1) (1.7)\r\n",
      "Requirement already satisfied: aliyun-python-sdk-kms>=2.4.1 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/aliyun_python_sdk_kms-2.15.0-py3.6.egg (from oss2>=2.8.0->alipai==0.3.4a1) (2.15.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages (from requests!=2.18.0,>=2.14.2->docker>=4.4.0->alipai==0.3.4a1) (2021.5.30)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/charset_normalizer-2.0.12-py3.6.egg (from requests!=2.18.0,>=2.14.2->docker>=4.4.0->alipai==0.3.4a1) (2.0.12)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/idna-3.3-py3.6.egg (from requests!=2.18.0,>=2.14.2->docker>=4.4.0->alipai==0.3.4a1) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/anaconda3/envs/pai-dev-py36/lib/python3.6/site-packages/urllib3-1.26.9-py3.6.egg (from requests!=2.18.0,>=2.14.2->docker>=4.4.0->alipai==0.3.4a1) (1.26.9)\r\n",
      "Installing collected packages: numpy, importlib-metadata\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.19.2\r\n",
      "    Uninstalling numpy-1.19.2:\r\n",
      "      Successfully uninstalled numpy-1.19.2\r\n",
      "  Attempting uninstall: importlib-metadata\r\n",
      "    Found existing installation: importlib-metadata 4.8.3\r\n",
      "    Uninstalling importlib-metadata-4.8.3:\r\n",
      "      Successfully uninstalled importlib-metadata-4.8.3\r\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pai-running-utils 0.4.0 requires alibabacloud-credentials>=0.1.4, which is not installed.\r\n",
      "pai-running-utils 0.4.0 requires alibabacloud_endpoint_util<1.0.0,>=0.0.3, which is not installed.\r\n",
      "pai-running-utils 0.4.0 requires alibabacloud_openapi_util<1.0.0,>=0.1.6, which is not installed.\r\n",
      "pai-running-utils 0.4.0 requires alibabacloud_tea_openapi<1.0.0,>=0.3.1, which is not installed.\r\n",
      "pai-running-utils 0.4.0 requires alibabacloud_tea_util<1.0.0,>=0.3.5, which is not installed.\r\n",
      "twine 3.8.0 requires importlib-metadata>=3.6, but you have importlib-metadata 2.1.0 which is incompatible.\r\n",
      "keyring 23.4.1 requires importlib-metadata>=3.6, but you have importlib-metadata 2.1.0 which is incompatible.\u001B[0m\r\n",
      "Successfully installed importlib-metadata-2.1.0 numpy-1.18.0\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install https://pai-sdk.oss-cn-shanghai.aliyuncs.com/alipai/dist/alipai-0.3.4a1-py2.py3-none-any.whl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 初始化默认的Session\n",
    "\n",
    "请在阿里云的控制台，获取使用的鉴权凭证和工作空间\n",
    "\n",
    "- AccessKeyId和AccessKeySecret\n",
    "\n",
    "请通过 [RAM控制台](https://ram.console.aliyun.com/manage/ak?spm=a2c8b.12215454.top-nav.dak.1704336aEeHgvy) 获取当前账号使用的AK信息\n",
    "\n",
    "- WorkspaceId\n",
    "\n",
    "通过 [PAI的控制台](https://pai.console.aliyun.com/?spm=a2c4g.11186623.0.0.506a7ba7JBg0qi&regionId=cn-hangzhou#/workspace/list) 查看你所在的AI工作空间ID.\n",
    "\n",
    "- OSS Bucket Name\n",
    "\n",
    "通过 [OSS控制台](https://oss.console.aliyun.com/) 查看可用的OSS Bucket，请确认使用的OSS region和工作空间是一致的。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pai\n",
    "\n",
    "print(pai.__version__)\n",
    "\n",
    "from pai.core.session import setup_default_session, Session\n",
    "\n",
    "sess = Session.current()\n",
    "\n",
    "if not sess:\n",
    "    print(\"config session\")\n",
    "    sess = setup_default_session(\n",
    "        access_key_id=\"<YourAccessKeyId>\",\n",
    "        access_key_secret=\"<YourAccessKeySecret>\",\n",
    "        region_id=\"<RegionIdWorking>\",\n",
    "        workspace_id=\"<YourWorkspaceId>\",\n",
    "        oss_bucket_name=\"<YourOssBucketName>\",\n",
    "    )\n",
    "    # 将当前的配置持久化到 ~/.pai/config.json，SDK默认从对应的路径读取配置初始化默认session。\n",
    "    sess.persist_config()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.4b1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 构建Conditional节点\n",
    "\n",
    "以下示例中，我们使用了一个自定义组件，组件会输出一个参数(output parameter)。这个输出参数，可以用于构建条件判断，支持用户构建一个条件节点(ConditionalStep)。仅条件满足之后，对应的条件节点才会执行。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_path_uri oss://lq-pai-test-1.oss-cn-hangzhou.aliyuncs.com/custom-job-example/output/\n",
      "Create pipeline run success (run_id: flow-pvmdij49ey7gq1i4xv), please visit the link below to view the run detail.\n",
      "https://pai.console.aliyun.com/console?regionId=cn-hangzhou#/studio/task/detail/flow-pvmdij49ey7gq1i4xv\n",
      "Wait for run workflow init\n",
      "Add Node Logger: ConditionalPipelineExample, node-xh707ytuqrootlc610\n",
      "Add Node Logger: ConditionalPipelineExample.step1, node-tegibpq0rv5w923r7r\n",
      "Add Node Logger: ConditionalPipelineExample.step3, node-vv62glrfzh4vuxs58m\n",
      "Add Node Logger: ConditionalPipelineExample.step2, node-n3tmd12nsvzlnxd75x\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.093351426+08:00 stderr F 2022/07/06 09:52:33 INFO: pai_running_utils: 0.4.0\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.093700524+08:00 stderr F 2022/07/06 09:52:33 INFO: Env PAI_SERVICE_ENV=\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.094948635+08:00 stderr F 2022/07/06 09:52:33 INFO: Env PAI_PIPELINE_VERSION=\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.095787635+08:00 stderr F 2022/07/06 09:52:33 INFO: Env PAI_PIPELINE_METADATA_IDENTIFIER=\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.097055265+08:00 stderr F 2022/07/06 09:52:33 INFO: Env PAI_PIPELINE_METADATA_VERSION=\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.097999182+08:00 stderr F 2022/07/06 09:52:33 INFO: Env PAI_PIPELINE_METADATA_PROVIDER=\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.098867804+08:00 stderr F 2022/07/06 09:52:33 INFO: Env PAI_PARENT_USER_ID=\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.098970155+08:00 stderr F 2022/07/06 09:52:33 INFO: Env PAI_RUN_ID=flow-pvmdij49ey7gq1i4xv\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.099219209+08:00 stderr F 2022/07/06 09:52:33 INFO: Env PAI_NODE_ID=node-tegibpq0rv5w923r7r\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.099389851+08:00 stderr F 2022/07/06 09:52:33 INFO: Env PAI_AI_WORKSPACE_ID=\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.099587212+08:00 stderr F 2022/07/06 09:52:33 INFO: Env PAI_INPUTS_PARAMETERS=\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.099788583+08:00 stderr F 2022/07/06 09:52:33 INFO: Env PAI_MANIFEST_SPEC_INPUTS=\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.100148607+08:00 stderr F 2022/07/06 09:52:33 INFO: Env PAI_MANIFEST_SPEC_OUTPUTS=\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.10236995+08:00 stderr F 2022/07/06 09:52:33 INFO: Env PAI_BASE_DIR=\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.1031756+08:00 stderr F 2022/07/06 09:52:33 INFO: Env PAI_REGION_ID=\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.103531965+08:00 stderr F 2022/07/06 09:52:33 INFO: Env PAI_ACCESS_SECRET=AccessKeySecret\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.103871563+08:00 stderr F 2022/07/06 09:52:33 INFO: Env PAI_ACCESS_KEY=TMP.xxxxxxxxxx8rUi\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.104212372+08:00 stderr F 2022/07/06 09:52:33 INFO: Initialize context from file.\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.105528764+08:00 stderr F 2022/07/06 09:52:33 INFO: input parameters: {'job_config': {'JobSpecs': [{'EcsSpec': 'ecs.c6.large', 'ImageUri': None, 'PodCount': 1, 'Type': 'Worker'}]}, 'output_path': 'oss://lq-pai-test-1.oss-cn-hangzhou.aliyuncs.com/custom-job-example/output/'}\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.107204375+08:00 stderr F 2022/07/06 09:52:33 INFO: CustomJobSpec:\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.107695715+08:00 stderr F 2022/07/06 09:52:33 INFO: Job spec image_uri=registry.cn-hangzhou.aliyuncs.com/pai-dlc/xgboost-training:1.6.0-cpu-py36-ubuntu18.04 job_config={'JobSpecs': [{'EcsSpec': 'ecs.c6.large', 'ImageUri': None, 'PodCount': 1, 'Type': 'Worker'}]} job_type=TFJob\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.108142759+08:00 stderr F 2022/07/06 09:52:33 INFO: Job entry_point=None source_files=None command=bash -c 'mkdir -p /ml/output/output_parameters/ && echo 0.99 > /ml/output/output_parameters/test_acc'\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.108308193+08:00 stderr F 2022/07/06 09:52:33 INFO: Job output_path=oss://lq-pai-test-1.oss-cn-hangzhou.aliyuncs.com/custom-job-example/output/ oss_role_arn=None oss_aliyun_uid=None\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.291665121+08:00 stderr F 2022/07/06 09:52:33 INFO: create dataset using workspace service API.\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.354333362+08:00 stderr F 2022/07/06 09:52:33 INFO: Prepare output dataset: dataset_id=d-9lfphbv1edq5sbm7i4, path=oss://lq-pai-test-1.oss-cn-hangzhou.aliyuncs.com/custom-job-example/output/\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.356050135+08:00 stderr F 2022/07/06 09:52:33 INFO: CreateJobRequest: {'DataSources': [{'DataSourceId': 'd-9lfphbv1edq5sbm7i4'}], 'DisplayName': 'custom_job-None', 'Envs': {'JOB_OUTPUT_PATH': '/ml/output/'}, 'JobSpecs': [{'EcsSpec': 'ecs.c6.large', 'Image': 'registry.cn-hangzhou.aliyuncs.com/pai-dlc/xgboost-training:1.6.0-cpu-py36-ubuntu18.04', 'PodCount': 1, 'Type': 'Worker'}], 'JobType': 'TFJob', 'ThirdpartyLibs': [], 'UserCommand': \"mkdir -p /ml/code && cd /ml/code  && bash -c 'mkdir -p /ml/output/output_parameters/ && echo 0.99 > /ml/output/output_parameters/test_acc'\", 'WorkspaceId': '28293'}\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.674605405+08:00 stderr F 2022/07/06 09:52:33 INFO: Job url: https://pai.console.aliyun.com/?regionId=cn-hangzhou&workspaceId=28293#/job/detail?jobId=dlc1qhdgcilvoinn\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:33.67602028+08:00 stderr F 2022/07/06 09:52:33 INFO: DlcJobUtils persist job id: dlc1qhdgcilvoinn\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:35.095095489+08:00 stderr F 2022/07/06 09:52:35 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:46.462356266+08:00 stderr F 2022/07/06 09:52:46 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:52:57.859671179+08:00 stderr F 2022/07/06 09:52:57 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:53:09.266763469+08:00 stderr F 2022/07/06 09:53:09 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:53:20.731463463+08:00 stderr F 2022/07/06 09:53:20 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:53:32.145989771+08:00 stderr F 2022/07/06 09:53:32 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:53:43.65236566+08:00 stderr F 2022/07/06 09:53:43 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:53:55.268769961+08:00 stderr F 2022/07/06 09:53:55 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:54:06.808156162+08:00 stderr F 2022/07/06 09:54:06 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:54:18.424537669+08:00 stderr F 2022/07/06 09:54:18 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:54:29.751947976+08:00 stderr F 2022/07/06 09:54:29 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:54:41.128580726+08:00 stderr F 2022/07/06 09:54:41 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:54:52.607208103+08:00 stderr F 2022/07/06 09:54:52 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:55:04.177684103+08:00 stderr F 2022/07/06 09:55:04 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:55:15.669341464+08:00 stderr F 2022/07/06 09:55:15 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:55:26.998368631+08:00 stderr F 2022/07/06 09:55:26 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:55:38.51063525+08:00 stderr F 2022/07/06 09:55:38 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:55:49.89946089+08:00 stderr F 2022/07/06 09:55:49 INFO: job status: Created\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:56:01.374174301+08:00 stderr F 2022/07/06 09:56:01 INFO: job status: Created\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:56:12.785266917+08:00 stderr F 2022/07/06 09:56:12 INFO: job status: Created\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:56:24.230171672+08:00 stderr F 2022/07/06 09:56:24 INFO: job status: Created\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:56:35.633789827+08:00 stderr F 2022/07/06 09:56:35 INFO: job status: Created\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:56:46.935213709+08:00 stderr F 2022/07/06 09:56:46 INFO: job status: Created\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:56:58.215313771+08:00 stderr F 2022/07/06 09:56:58 INFO: job status: Created\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:57:09.524856765+08:00 stderr F 2022/07/06 09:57:09 INFO: job status: Created\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:57:20.894907456+08:00 stderr F 2022/07/06 09:57:20 INFO: job status: Created\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:57:32.153095233+08:00 stderr F 2022/07/06 09:57:32 INFO: job status: Created\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:57:43.591624461+08:00 stderr F 2022/07/06 09:57:43 INFO: job status: Created\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:57:55.062967254+08:00 stderr F 2022/07/06 09:57:55 INFO: job status: Created\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:58:06.627105785+08:00 stderr F 2022/07/06 09:58:06 INFO: job status: Succeeded\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:58:06.736998385+08:00 stderr F 2022/07/06 09:58:06 INFO: CustomJob output parameter: test_acc=0.99\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:58:06.73714027+08:00 stderr F \n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:58:06.737768584+08:00 stderr F 2022/07/06 09:58:06 INFO: CustomJob execution succeed.\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:58:06.73811543+08:00 stderr F 2022/07/06 09:58:06 INFO: CustomJobExecutor delete created tmp dataset ids=['d-9lfphbv1edq5sbm7i4']\n",
      "ConditionalPipelineExample.step1: 2022-07-06T17:58:06.738250119+08:00 stderr F 2022/07/06 09:58:06 INFO: delete dataset using workspace service api\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.478726759+08:00 stderr F 2022/07/06 09:59:45 INFO: pai_running_utils: 0.4.0\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.478937222+08:00 stderr F 2022/07/06 09:59:45 INFO: Env PAI_SERVICE_ENV=\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.479513742+08:00 stderr F 2022/07/06 09:59:45 INFO: Env PAI_PIPELINE_VERSION=\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.479841934+08:00 stderr F 2022/07/06 09:59:45 INFO: Env PAI_PIPELINE_METADATA_IDENTIFIER=\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.480675139+08:00 stderr F 2022/07/06 09:59:45 INFO: Env PAI_PIPELINE_METADATA_VERSION=\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.481488761+08:00 stderr F 2022/07/06 09:59:45 INFO: Env PAI_PIPELINE_METADATA_PROVIDER=\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.481835885+08:00 stderr F 2022/07/06 09:59:45 INFO: Env PAI_PARENT_USER_ID=\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.482718604+08:00 stderr F 2022/07/06 09:59:45 INFO: Env PAI_RUN_ID=flow-pvmdij49ey7gq1i4xv\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.483023676+08:00 stderr F 2022/07/06 09:59:45 INFO: Env PAI_NODE_ID=node-n3tmd12nsvzlnxd75x\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.483856682+08:00 stderr F 2022/07/06 09:59:45 INFO: Env PAI_AI_WORKSPACE_ID=\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.484655538+08:00 stderr F 2022/07/06 09:59:45 INFO: Env PAI_INPUTS_PARAMETERS=\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.485537457+08:00 stderr F 2022/07/06 09:59:45 INFO: Env PAI_MANIFEST_SPEC_INPUTS=\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.486313913+08:00 stderr F 2022/07/06 09:59:45 INFO: Env PAI_MANIFEST_SPEC_OUTPUTS=\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.487188933+08:00 stderr F 2022/07/06 09:59:45 INFO: Env PAI_BASE_DIR=\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.487347842+08:00 stderr F 2022/07/06 09:59:45 INFO: Env PAI_REGION_ID=\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.487491211+08:00 stderr F 2022/07/06 09:59:45 INFO: Env PAI_ACCESS_KEY=TMP.xxxxxxxxxx5VXa\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.488607143+08:00 stderr F 2022/07/06 09:59:45 INFO: Env PAI_ACCESS_SECRET=AccessKeySecret\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.488861508+08:00 stderr F 2022/07/06 09:59:45 INFO: Initialize context from file.\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.492841359+08:00 stderr F 2022/07/06 09:59:45 INFO: input parameters: {'output_path': '', 'job_config': {'JobSpecs': [{'EcsSpec': 'ecs.c6.large', 'ImageUri': None, 'PodCount': 1, 'Type': 'Worker'}]}}\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.494571536+08:00 stderr F 2022/07/06 09:59:45 INFO: CustomJobSpec:\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.494970833+08:00 stderr F 2022/07/06 09:59:45 INFO: Job spec image_uri=registry.cn-hangzhou.aliyuncs.com/pai-dlc/xgboost-training:1.6.0-cpu-py36-ubuntu18.04 job_config={'JobSpecs': [{'EcsSpec': 'ecs.c6.large', 'ImageUri': None, 'PodCount': 1, 'Type': 'Worker'}]} job_type=TFJob\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.495091464+08:00 stderr F 2022/07/06 09:59:45 INFO: Job entry_point=None source_files=None command=bash -c 'mkdir -p /ml/output/output_parameters/ && echo 0.99 > /ml/output/output_parameters/test_acc'\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.496242187+08:00 stderr F 2022/07/06 09:59:45 INFO: Job output_path= oss_role_arn=None oss_aliyun_uid=None\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.656600566+08:00 stderr F 2022/07/06 09:59:45 INFO: CreateJobRequest: {'DataSources': [], 'DisplayName': 'custom_job-None', 'Envs': {'JOB_OUTPUT_PATH': '/ml/output/'}, 'JobSpecs': [{'EcsSpec': 'ecs.c6.large', 'Image': 'registry.cn-hangzhou.aliyuncs.com/pai-dlc/xgboost-training:1.6.0-cpu-py36-ubuntu18.04', 'PodCount': 1, 'Type': 'Worker'}], 'JobType': 'TFJob', 'ThirdpartyLibs': [], 'UserCommand': \"mkdir -p /ml/code && cd /ml/code  && bash -c 'mkdir -p /ml/output/output_parameters/ && echo 0.99 > /ml/output/output_parameters/test_acc'\", 'WorkspaceId': '28293'}\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.891243633+08:00 stderr F 2022/07/06 09:59:45 INFO: Job url: https://pai.console.aliyun.com/?regionId=cn-hangzhou&workspaceId=28293#/job/detail?jobId=dlc14acqv6aejr11\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:45.892513994+08:00 stderr F 2022/07/06 09:59:45 INFO: DlcJobUtils persist job id: dlc14acqv6aejr11\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:47.61084473+08:00 stderr F 2022/07/06 09:59:47 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step2: 2022-07-06T17:59:59.126473906+08:00 stderr F 2022/07/06 09:59:59 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:00:11.552776831+08:00 stderr F 2022/07/06 10:00:11 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:00:23.037988666+08:00 stderr F 2022/07/06 10:00:23 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:00:34.533797238+08:00 stderr F 2022/07/06 10:00:34 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:00:46.043132958+08:00 stderr F 2022/07/06 10:00:46 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:00:57.55110291+08:00 stderr F 2022/07/06 10:00:57 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:01:09.236888026+08:00 stderr F 2022/07/06 10:01:09 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:01:23.508199576+08:00 stderr F 2022/07/06 10:01:23 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:01:35.273184232+08:00 stderr F 2022/07/06 10:01:35 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:01:46.753185089+08:00 stderr F 2022/07/06 10:01:46 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:01:59.205916807+08:00 stderr F 2022/07/06 10:01:59 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:02:10.600306263+08:00 stderr F 2022/07/06 10:02:10 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:02:21.935736847+08:00 stderr F 2022/07/06 10:02:21 INFO: job status: Creating\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:02:33.300324002+08:00 stderr F 2022/07/06 10:02:33 INFO: job status: Created\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:02:44.766275809+08:00 stderr F 2022/07/06 10:02:44 INFO: job status: Created\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:02:56.197870913+08:00 stderr F 2022/07/06 10:02:56 INFO: job status: Created\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:07.777785204+08:00 stderr F 2022/07/06 10:03:07 INFO: job status: Created\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:19.135558799+08:00 stderr F 2022/07/06 10:03:19 INFO: job status: Created\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:30.737536447+08:00 stderr F 2022/07/06 10:03:30 INFO: job status: Created\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:42.144295186+08:00 stderr F 2022/07/06 10:03:42 INFO: job status: Succeeded\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:42.146749315+08:00 stderr F Traceback (most recent call last):\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:42.14683338+08:00 stderr F   File \"/miniconda/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:42.146918926+08:00 stderr F     \"__main__\", mod_spec)\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:42.146980025+08:00 stderr F   File \"/miniconda/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:42.147039717+08:00 stderr F     exec(code, run_globals)\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:42.147108846+08:00 stderr F   File \"/miniconda/lib/python3.7/site-packages/pai_running/cmd/custom_job_submit.py\", line 103, in <module>\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:42.149899663+08:00 stderr F     custom_job_submit()\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:42.149998519+08:00 stderr F   File \"/miniconda/lib/python3.7/site-packages/pai_running/cmd/custom_job_submit.py\", line 99, in custom_job_submit\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:42.151647664+08:00 stderr F     executor.submit()\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:42.151737717+08:00 stderr F   File \"/miniconda/lib/python3.7/site-packages/pai_running/executor/custom_job_executor.py\", line 392, in submit\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:42.153370363+08:00 stderr F     self.on_success()\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:42.15346892+08:00 stderr F   File \"/miniconda/lib/python3.7/site-packages/pai_running/executor/custom_job_executor.py\", line 397, in on_success\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:42.153594886+08:00 stderr F     self.write_output_parameters()\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:42.153666439+08:00 stderr F   File \"/miniconda/lib/python3.7/site-packages/pai_running/executor/custom_job_executor.py\", line 418, in write_output_parameters\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:42.153767908+08:00 stderr F     oss_bucket = self.get_job_output_oss_bucket()\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:42.153844885+08:00 stderr F   File \"/miniconda/lib/python3.7/site-packages/pai_running/executor/custom_job_executor.py\", line 401, in get_job_output_oss_bucket\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:42.153916965+08:00 stderr F     bucket_name, object_key, endpoint = parse_oss_url(self.job_output_path)\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:42.153992519+08:00 stderr F   File \"/miniconda/lib/python3.7/site-packages/pai_running/utils.py\", line 19, in parse_oss_url\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:42.154083651+08:00 stderr F     raise ValueError(\"require oss url but given '{}'\".format(oss_url))\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:42.154155821+08:00 stderr F ValueError: require oss url but given ''\n",
      "ConditionalPipelineExample.step2: 2022-07-06T18:03:42.154257422+08:00 stderr F 2022/07/06 10:03:42 INFO: CustomJobExecutor delete created tmp dataset ids=[]\n"
     ]
    },
    {
     "ename": "PAIException",
     "evalue": "PipelineRun failed: run_id=flow-pvmdij49ey7gq1i4xv, run_status_info={'ConditionalPipelineExample': {'name': 'tmp-2lziczwrjtkhg44o-pvmdij49ey7gq1i4xv', 'nodeId': 'node-xh707ytuqrootlc610', 'status': 'Failed', 'startedAt': '2022-07-06T09:49:45.000Z', 'finishedAt': '2022-07-06T10:04:22.000Z'}, 'ConditionalPipelineExample.step1': {'name': 'step1', 'nodeId': 'node-tegibpq0rv5w923r7r', 'status': 'Succeeded', 'startedAt': '2022-07-06T09:49:45.000Z', 'finishedAt': '2022-07-06T09:58:38.000Z'}, 'ConditionalPipelineExample.step3': {'name': 'step3', 'nodeId': 'node-vv62glrfzh4vuxs58m', 'status': 'Skipped', 'startedAt': '2022-07-06T09:58:48.000Z', 'finishedAt': '2022-07-06T09:58:48.000Z'}, 'ConditionalPipelineExample.step2': {'name': 'step2', 'nodeId': 'node-n3tmd12nsvzlnxd75x', 'status': 'Failed', 'startedAt': '2022-07-06T09:58:48.000Z', 'finishedAt': '2022-07-06T10:04:12.000Z'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPAIException\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-44955a562fc4>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[0mp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPipeline\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msteps\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mstep3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstep2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstep1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 61\u001B[0;31m \u001B[0mp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"ConditionalPipelineExample\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     62\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/code/pypai/pai/operator/_base.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, job_name, wait, arguments, show_outputs, **kwargs)\u001B[0m\n\u001B[1;32m    163\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mwait\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    164\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mrun_instance\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 165\u001B[0;31m         \u001B[0mrun_instance\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait_for_completion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mshow_outputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mshow_outputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    166\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mrun_instance\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    167\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/code/pypai/pai/pipeline/run.py\u001B[0m in \u001B[0;36mwait_for_completion\u001B[0;34m(self, show_outputs, timeout)\u001B[0m\n\u001B[1;32m    327\u001B[0m             \u001B[0mlogger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdebug\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"catch expections, %s\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m             \u001B[0mrun_logger\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstop_tail\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 329\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    330\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    331\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mlog_runner\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mlog_runners\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/code/pypai/pai/pipeline/run.py\u001B[0m in \u001B[0;36mwait_for_completion\u001B[0;34m(self, show_outputs, timeout)\u001B[0m\n\u001B[1;32m    306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    307\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mroot_node_status\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mPipelineRunStatus\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mFailed\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 308\u001B[0;31m                     raise PAIException(\n\u001B[0m\u001B[1;32m    309\u001B[0m                         \"PipelineRun failed: run_id={}, run_status_info={}\".format(\n\u001B[1;32m    310\u001B[0m                             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcurr_status_infos\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mPAIException\u001B[0m: PipelineRun failed: run_id=flow-pvmdij49ey7gq1i4xv, run_status_info={'ConditionalPipelineExample': {'name': 'tmp-2lziczwrjtkhg44o-pvmdij49ey7gq1i4xv', 'nodeId': 'node-xh707ytuqrootlc610', 'status': 'Failed', 'startedAt': '2022-07-06T09:49:45.000Z', 'finishedAt': '2022-07-06T10:04:22.000Z'}, 'ConditionalPipelineExample.step1': {'name': 'step1', 'nodeId': 'node-tegibpq0rv5w923r7r', 'status': 'Succeeded', 'startedAt': '2022-07-06T09:49:45.000Z', 'finishedAt': '2022-07-06T09:58:38.000Z'}, 'ConditionalPipelineExample.step3': {'name': 'step3', 'nodeId': 'node-vv62glrfzh4vuxs58m', 'status': 'Skipped', 'startedAt': '2022-07-06T09:58:48.000Z', 'finishedAt': '2022-07-06T09:58:48.000Z'}, 'ConditionalPipelineExample.step2': {'name': 'step2', 'nodeId': 'node-n3tmd12nsvzlnxd75x', 'status': 'Failed', 'startedAt': '2022-07-06T09:58:48.000Z', 'finishedAt': '2022-07-06T10:04:12.000Z'}}"
     ]
    }
   ],
   "source": [
    "from pai.job.common import JobConfig\n",
    "from pai.operator.types import PipelineParameter\n",
    "from pai.operator import CustomJobOperator\n",
    "from pai.pipeline import Pipeline\n",
    "\n",
    "# 自定义节点使用的镜像，这里我们使用了PAI仓库内提供的XGBoost社区镜像运行我们的任务。\n",
    "image_uri = \"registry.{}.aliyuncs.com/pai-dlc/xgboost-training:1.6.0-cpu-py36-ubuntu18.04\".format(\n",
    "    sess.region_id\n",
    ")\n",
    "\n",
    "\n",
    "output_path_uri = \"oss://{bucket_name}.{endpoint}/custom-job-example/output/\".format(\n",
    "    bucket_name=sess.oss_bucket.bucket_name,\n",
    "    endpoint=sess.oss_bucket.endpoint.strip(\"https://\"),\n",
    ")\n",
    "print(\"output_path_uri\", output_path_uri)\n",
    "\n",
    "\n",
    "# 这里我们构建自定义组件，会写出一个 test_acc 的output_parameter.\n",
    "# 这里依赖于我们的命令，或是脚本，将相应的输出参数，写出到 `/ml/output/output_parameters/<OutputParameterName>`\n",
    "output_param_name = \"test_acc\"\n",
    "op = CustomJobOperator(\n",
    "    outputs=[PipelineParameter(name=output_param_name)],\n",
    "    image_uri=image_uri,\n",
    "    command=[\n",
    "        \"bash\",\n",
    "        \"-c\",\n",
    "        \"mkdir -p /ml/output/output_parameters/ && echo 0.99 > /ml/output/output_parameters/%s\"\n",
    "        % output_param_name,\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "# 构建Pipeline中的第一个节点.\n",
    "step1 = op.as_step(\n",
    "    name=\"step1\",\n",
    "    inputs={\n",
    "        \"job_config\": JobConfig.create(\n",
    "            worker_count=1, worker_instance_type=\"ecs.c6.large\"\n",
    "        ).to_dict(),\n",
    "        \"output_path\": output_path_uri + \"step1_output/\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# 构建Pipeline中的第二个节点\n",
    "# 只有上游的output参数(step.outputs.test_acc) 大于 0.8时，才会执行当前节点。\n",
    "step2 = op.as_condition_step(\n",
    "    name=\"step2\",\n",
    "    condition=step1.outputs[0] > 0.8,\n",
    "    inputs={\n",
    "        \"job_config\": JobConfig.create(\n",
    "            worker_count=1, worker_instance_type=\"ecs.c6.large\"\n",
    "        ).to_dict(),\n",
    "        \"output_path\": output_path_uri + \"step2_output/\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# 构建Pipeline中的第三个节点\n",
    "# 只有上游的output参数(step.outputs.test_acc) 小于 0.8时，才会执行当前节点。\n",
    "step3 = op.as_condition_step(\n",
    "    name=\"step3\",\n",
    "    condition=step1.outputs[0] <= 0.8,\n",
    "    inputs={\n",
    "        \"job_config\": JobConfig.create(\n",
    "            worker_count=1, worker_instance_type=\"ecs.c6.large\"\n",
    "        ).to_dict(),\n",
    "        \"output_path\": output_path_uri + \"step3_output/\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# 构建对应的工作流\n",
    "# 不满足条件的相应节点，会被跳过(状态：skipped）\n",
    "p = Pipeline(steps=[step3, step2, step1])\n",
    "\n",
    "p.run(\"ConditionalPipelineRun\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
