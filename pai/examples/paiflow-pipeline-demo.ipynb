{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本期的SDK的主要功能包括：\n",
    "- 对目前的PaiFlow的后端接口的封装, 用户可以通过Session对象暴露的接口操作。\n",
    "- 用户使用Pipeline/Run class，在SDK端完成Pipeline的拼接，任务提交，运行查看。\n",
    "\n",
    "- 对于常用的算法组件的封装: 进行中\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/Users/liangquan/code/pypai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pai\n",
    "print(pai.__file__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import time\n",
    "import yaml\n",
    "from pai import Session, Pipeline, RunInstance, ProviderAlibabaPAI\n",
    "from pai.pipeline.parameter import ParameterType\n",
    "\n",
    "# 默认传的环境参数，在运行时如果env为none，则使用session的default_env\n",
    "default_env = {\"resource\": {\n",
    "        \"compute\": {\n",
    "            \"max_compute\": {\n",
    "            \"accessKey\": \"3AccessKeySecret\",\n",
    "            \"accessId\": \"AccessKeyId\",\n",
    "                \"endpoint\": \"http://service.cn-shanghai.maxcompute.aliyun.com/api\",\n",
    "                \"logViewHost\": \"http://logview.odps.aliyun.com\",\n",
    "                \"odpsProject\": \"wyl_test\",\n",
    "            },\n",
    "       }\n",
    "    }\n",
    "              }\n",
    "\n",
    "# 阿里云账号AK信息\n",
    "config = {\n",
    "    \"access_key_id\": \"AccessKeyId\",\n",
    "    \"access_key_secret\": \"3AccessKeySecret\",\n",
    "    \"region_id\": \"cn-shanghai\",\n",
    "    \"odps_project\": \"wyl_test\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用户通过search_pipeline搜索服务端的Pipeline\n",
    "- 模糊搜索PAI提供的，identifier包含`ODPS`的Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines, total_count = session.search_pipeline(identifier=\"ODPS\", fuzzy=True, provider=ProviderAlibabaPAI, page_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for pipeline in pipelines:\n",
    "    print(pipeline[\"Identifier\"], pipeline[\"Version\"], pipeline[\"Provider\"], pipeline[\"PipelineId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipelines, total_count = session.search_pipeline(provider=ProviderAlibabaPAI, page_size=100)\n",
    "for pipeline in pipelines:\n",
    "    print(pipeline[\"Identifier\"], pipeline[\"Version\"], pipeline[\"Provider\"], pipeline[\"PipelineId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- session可以通过具体的PipelineID拉取Pipeline的信息\n",
    "- 也可以通过(identifier, provider, version)的3元组信息拉取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_pipeline_by_id(pipelines[0][\"PipelineId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.get_pipeline(\"evaluate-xflow-maxCompute\", version=\"v1\", provider=ProviderAlibabaPAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 用户直接运行从服务端拉取的Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_info = session.get_pipeline(identifier=\"dataSource-xflow-maxCompute\", provider=ProviderAlibabaPAI, version=\"v1\")\n",
    "pipeline_id = pipeline_info[\"PipelineId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {\"parameters\": [\n",
    "    {\n",
    "        \"name\": \"execution\",\n",
    "        \"value\": {\n",
    "            \"odpsInfoFile\": \"/share/base/odpsInfo.ini\",\n",
    "            \"endpoint\": \"http://service.cn-shanghai.maxcompute.aliyun.com/api\",\n",
    "            \"logViewHost\": \"http://logview.odps.aliyun.com\",\n",
    "            \"odpsProject\": \"wyl_test\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"tableName\",\n",
    "        \"value\": \"pai_online_project.wumai_data\",\n",
    "    },\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_id = session.create_pipeline_run(\"demo_paiflow\",\n",
    "                                      pipeline_id=pipeline_id,\n",
    "                                      arguments=arguments,\n",
    "                                      no_confirm_required=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = RunInstance(run_id=run_id, session=session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###  用户构造复合的Pipeline，推送服务端保存，或是直接运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_simple_composite_pipeline(session):\n",
    "    \"\"\"Composite data_source and type_transform pipeline\"\"\"\n",
    "\n",
    "    version = \"v%s\" % (str(int(time.time() * 1000)))\n",
    "    \n",
    "    # 初始化一个Pipeline模板, 声明Pipeline的名称和版本\n",
    "    # 提供Session对象，用户后续拉取服务端的模板，以及提供当前阿里云主账号信息\n",
    "    p = Pipeline.new_pipeline(identifier=\"demo-data-source-type-transform\", version=version,\n",
    "                              session=session)\n",
    "\n",
    "    \n",
    "    # 指定需要创建的Pipeline的输入\n",
    "    execution_input = p.create_input_parameter(\"execution\", 'map', required=True)\n",
    "    cols_to_double_input = p.create_input_parameter(\"cols_to_double\", str, required=True)\n",
    "    input_table_name = p.create_input_parameter(\"table_name\", str, required=True)\n",
    "    # hist_cols_input = p.create_input_parameter(\"histogram_selected_col_names\", str, required=True)\n",
    "    \n",
    "    \n",
    "    # 添加一个odps-data-source step\n",
    "    data_source_step = p.create_step(\"dataSource-xflow-maxCompute\", provider=ProviderAlibabaPAI, version=\"v1\", name=\"dataSource\")\n",
    "    \n",
    "    # 指定dataSource的数据输入来源\n",
    "    data_source_step.set_arguments(\n",
    "        execution=execution_input,\n",
    "        tableName=input_table_name,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # 指定typeTransform的数据来源\n",
    "    type_transform_step = p.create_step(\"type-transform-xflow-maxCompute\", provider=ProviderAlibabaPAI, version=\"v1\", name=\"typeTransform\")\n",
    "    type_transform_step.set_arguments(\n",
    "        inputArtifact=data_source_step.outputs[\"outputArtifact\"],\n",
    "        execution=execution_input,\n",
    "#         outputTable=\"type-transform-xflow-ODPS\",\n",
    "        outputTable=\"pai_temp_123455677_18188283\",\n",
    "        cols_to_double=cols_to_double_input,\n",
    "    )\n",
    "    \n",
    "\n",
    "    \n",
    "    # 设定pipeline的输出，以及输出的来源\n",
    "    p.create_output_artifact(\"outputArtifact\", type_transform_step.outputs[\"outputArtifact\"])\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = create_simple_composite_pipeline(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.dot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def args_for_composite_pipeline_1():\n",
    "    arguments = {\"execution\":{\n",
    "            \"accessKey\": \"3AccessKeySecret\",\n",
    "            \"accessId\": \"AccessKeyId\",\n",
    "                \"endpoint\": \"http://service.cn-shanghai.maxcompute.aliyun.com/api\",\n",
    "                \"logViewHost\": \"http://logview.odps.aliyun.com\",\n",
    "                \"odpsProject\": \"wyl_test\",\n",
    "            },\n",
    "        \"cols_to_double\":\"time,hour,pm2,pm10,so2,co,no2\",\n",
    "        \"table_name\":\"pai_online_project.wumai_data\"\n",
    "    }\n",
    "\n",
    "\n",
    "    return arguments, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "两种方案提交Pipeline运行.\n",
    "\n",
    "- 提交Pipeline的Manifest直接创建一个Run运行任务\n",
    "- 上传Pipeline Manifest到服务端，获得PipelineID之后，通过指定PipelineID运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "arguments, env = args_for_composite_pipeline_1()\n",
    "# wait为False的话直接返回\n",
    "run_instance = p.run(\"demo_temp_pipeline_run\", arguments=arguments, wait=False, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_instance.get_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_instance.wait(log_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_instance.get_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.list_pipeline_run(page_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 一个复杂一点的复合Pipeline的构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_air_quality_prediction(session):\n",
    "    # version = \"v%s\" % (str(int(time.time() * 1000)))\n",
    "    p = Pipeline.new_pipeline(\"ut-air-quality\", version=\"v1.0.0\", session=session)\n",
    "\n",
    "    execution_input = p.create_input_parameter(\"execution\", \"map\", required=True)\n",
    "    cols_to_double_input = p.create_input_parameter(\"cols_to_double\", str, required=True)\n",
    "    hist_cols_input = p.create_input_parameter(\"histogram_selected_col_names\", str,\n",
    "                                                required=True)\n",
    "    sql_input = p.create_input_parameter(\"sql\", str, required=True)\n",
    "    normalize_cols_input = p.create_input_parameter(\"normalize_selected_col_names\", str,\n",
    "                                                    required=True)\n",
    "    fraction_input = p.create_input_parameter(\"fraction\", float, required=True)\n",
    "    randomforest_feature_cols_input = p.create_input_parameter(\"randomforest_feature_col_names\",\n",
    "                                                                str, required=True)\n",
    "    randomforest_label_col_input = p.create_input_parameter(\"randomforest_label_col_names\", str,\n",
    "                                                            required=True)\n",
    "    prediction1_feature_col_input = p.create_input_parameter(\"prediction1_feature_col_names\",\n",
    "                                                                str, required=True)\n",
    "    prediction1_append_col_input = p.create_input_parameter(\"prediction1_append_col_names\", str,\n",
    "                                                            required=True)\n",
    "    prediction1_result_col_input = p.create_input_parameter(\"prediction1_result_col_names\", str,\n",
    "                                                            required=True)\n",
    "    prediction1_score_col_input = p.create_input_parameter(\"prediction1_score_col_names\", str,\n",
    "                                                            required=True)\n",
    "    prediction1_detail_col_input = p.create_input_parameter(\"prediction1_detail_col_names\", str,\n",
    "                                                            required=True)\n",
    "\n",
    "    evaluate1_label_col_input = p.create_input_parameter(\"evaluate1_label_col_name\", str,\n",
    "                                                            required=True)\n",
    "    evaluate1_score_col_input = p.create_input_parameter(\"evaluate1_score_col_name\", str,\n",
    "                                                            required=True)\n",
    "    evaluate1_positive_label_input = p.create_input_parameter(\"evaluate1_positive_label\", int,\n",
    "                                                                required=True)\n",
    "    evaluate1_bin_count_input = p.create_input_parameter(\"evaluate1_bin_count\", int,\n",
    "                                                            required=True)\n",
    "\n",
    "    logistic_feature_col_input = p.create_input_parameter(\n",
    "        \"logisticregression_feature_col_names\", str,\n",
    "        required=True)\n",
    "    logistic_label_col_names = p.create_input_parameter(\"logisticregression_label_col_names\",\n",
    "                                                        str, required=True)\n",
    "    logistic_good_value_input = p.create_input_parameter(\"logisticregression_good_value\", int,\n",
    "                                                            required=True)\n",
    "\n",
    "    prediction2_feature_col_input = p.create_input_parameter(\"prediction2_feature_col_names\",\n",
    "                                                                str, required=True)\n",
    "    prediction2_append_col_input = p.create_input_parameter(\"prediction2_append_col_names\", str,\n",
    "                                                            required=True)\n",
    "    prediction2_result_col_input = p.create_input_parameter(\"prediction2_result_col_names\", str,\n",
    "                                                            required=True)\n",
    "    prediction2_score_col_input = p.create_input_parameter(\"prediction2_score_col_names\", str,\n",
    "                                                            required=True)\n",
    "    prediction2_detail_col_input = p.create_input_parameter(\"prediction2_detail_col_names\", str,\n",
    "                                                            required=True)\n",
    "\n",
    "    evaluate2_label_col_input = p.create_input_parameter(\"evaluate2_label_col_name\", str,\n",
    "                                                            required=True)\n",
    "    evaluate2_score_col_input = p.create_input_parameter(\"evaluate2_score_col_name\", str,\n",
    "                                                            required=True)\n",
    "    evaluate2_positive_label_input = p.create_input_parameter(\"evaluate2_positive_label\", int,\n",
    "                                                                required=True)\n",
    "    evaluate2_bin_count_input = p.create_input_parameter(\"evaluate2_bin_count\", int,\n",
    "                                                            required=True)\n",
    "\n",
    "    data_source_step = p.create_step(\"dataSource-xflow-maxCompute\",\n",
    "                                        provider=ProviderAlibabaPAI,\n",
    "                                        name=\"dataSource\")\n",
    "\n",
    "    data_source_step.set_arguments(\n",
    "        execution=execution_input,\n",
    "        tableName=\"pai_online_project.wumai_data\",\n",
    "    )\n",
    "\n",
    "    type_transform_step = p.create_step(\"type-transform-xflow-maxCompute\",\n",
    "                                        provider=ProviderAlibabaPAI,\n",
    "                                        name=\"typeTransform\")\n",
    "    type_transform_step.set_arguments(\n",
    "        inputArtifact=data_source_step.outputs[\"outputArtifact\"],\n",
    "        execution=execution_input,\n",
    "        outputTable=\"type-transform-xflow-maxCompute\",\n",
    "        cols_to_double=cols_to_double_input,\n",
    "    )\n",
    "\n",
    "    histogram_step = p.create_step(\"histogram-xflow-maxCompute\",\n",
    "                                    provider=ProviderAlibabaPAI,\n",
    "                                    name=\"histogram\")\n",
    "    histogram_step.set_arguments(\n",
    "        inputArtifact=type_transform_step.outputs[\"outputArtifact\"],\n",
    "        execution=execution_input,\n",
    "        outputTableName=\"pai_temp_172808_1779985_1\",\n",
    "        selectedColNames=hist_cols_input,\n",
    "    )\n",
    "\n",
    "    sql_step = p.create_step(\"sql-xflow-maxCompute\",\n",
    "                                provider=ProviderAlibabaPAI,\n",
    "                                name=\"sql\")\n",
    "    sql_step.set_arguments(\n",
    "        inputArtifact1=type_transform_step.outputs[\"outputArtifact\"],\n",
    "        execution=execution_input,\n",
    "        outputTable=\"pai_temp_83935_1099579_1\",\n",
    "        sql=sql_input,\n",
    "    )\n",
    "\n",
    "    fe_meta_runner_step = p.create_step(\"fe-meta-runner-xflow-maxCompute\",\n",
    "                                        provider=ProviderAlibabaPAI,\n",
    "                                        name=\"feMetaRunner\")\n",
    "\n",
    "    fe_meta_runner_step.set_arguments(\n",
    "        inputArtifact=sql_step.outputs[\"outputArtifact\"],\n",
    "        execution=execution_input,\n",
    "        outputTable=\"pai_temp_83935_1099581_1\",\n",
    "        mapTable=\"pai_temp_83935_1099581_2\",\n",
    "        selectedCols=\"pm10,so2,co,no2\",\n",
    "        labelCol=\"_c2\",\n",
    "    )\n",
    "\n",
    "    normalized_step = p.create_step(\"normalize-xflow-maxCompute\",\n",
    "                                    provider=ProviderAlibabaPAI,\n",
    "                                    name=\"normalize\")\n",
    "    normalized_step.set_arguments(\n",
    "        inputArtifact=sql_step.outputs[\"outputArtifact\"],\n",
    "        execution=execution_input,\n",
    "        outputTableName=\"pai_temp_83935_1099582_1\",\n",
    "        outputParaTableName=\"pai_temp_83935_1099582_2\",\n",
    "        selectedColNames=normalize_cols_input,\n",
    "    )\n",
    "\n",
    "    split_step = p.create_step(\"split-xflow-maxCompute\",\n",
    "                                provider=ProviderAlibabaPAI,\n",
    "                                name=\"split\")\n",
    "    split_step.set_arguments(\n",
    "        inputArtifact=normalized_step.outputs[\"outputArtifact\"],\n",
    "        execution=execution_input,\n",
    "        output1TableName=\"pai_temp_83935_1099583_1\",\n",
    "        fraction=fraction_input,\n",
    "        output2TableName=\"pai_temp_83935_1199583_1\",\n",
    "    )\n",
    "\n",
    "    randomforest_step = p.create_step(\"randomforests-xflow-maxCompute\",\n",
    "                                        provider=ProviderAlibabaPAI,\n",
    "                                        name=\"randomforests\")\n",
    "    randomforest_step.set_arguments(\n",
    "        inputArtifact=split_step.outputs[\"outputArtifact1\"],\n",
    "        execution=execution_input,\n",
    "        featureColNames=randomforest_feature_cols_input,\n",
    "        labelColName=randomforest_label_col_input,\n",
    "        treeNum=100,\n",
    "        modelName=\"xlab_m_random_forests_1099584_v0\",\n",
    "    )\n",
    "\n",
    "    prediction1_step = p.create_step(\"prediction-xflow-maxCompute\",\n",
    "                                        provider=ProviderAlibabaPAI,\n",
    "                                        name=\"prediction1\")\n",
    "    prediction1_step.set_arguments(\n",
    "        inputModelArtifact=randomforest_step.outputs[\"outputArtifact\"],\n",
    "        inputDataSetArtifact=split_step.outputs[\"outputArtifact2\"],\n",
    "        execution=execution_input,\n",
    "        outputTableName=\"pai_temp_83935_1029583_1\",\n",
    "        featureColNames=prediction1_feature_col_input,\n",
    "        appendColNames=prediction1_append_col_input,\n",
    "        resultColName=prediction1_result_col_input,\n",
    "        scoreColName=prediction1_score_col_input,\n",
    "        detailColName=prediction1_detail_col_input,\n",
    "    )\n",
    "    evaluate1_step = p.create_step(\"evaluate-xflow-maxCompute\",\n",
    "                                    provider=ProviderAlibabaPAI,\n",
    "                                    name=\"evaluate1\")\n",
    "    evaluate1_step.set_arguments(\n",
    "        inputArtifact=prediction1_step.outputs[\"outputArtifact\"],\n",
    "        execution=execution_input,\n",
    "        outputDetailTableName=\"pai_temp_83935_1099586_1\",\n",
    "        outputMetricTableName=\"pai_temp_83935_1228529_1\",\n",
    "        outputELDetailTableName=\"pai_temp_83935_1299589_1\",\n",
    "        labelColName=evaluate1_label_col_input,\n",
    "        scoreColName=evaluate1_score_col_input,\n",
    "        positiveLabel=evaluate1_positive_label_input,\n",
    "        binCount=evaluate1_bin_count_input,\n",
    "    )\n",
    "    logistic_step = p.create_step(\"logisticregression-binary-xflow-maxCompute\",\n",
    "                                    provider=ProviderAlibabaPAI,\n",
    "                                    name=\"logisticregression\")\n",
    "\n",
    "    logistic_step.set_arguments(\n",
    "        inputArtifact=split_step.outputs[\"outputArtifact1\"],\n",
    "        execution=execution_input,\n",
    "        modelName=\"xlab_m_logisticregres_1099587_v0\",\n",
    "        featureColNames=logistic_feature_col_input,\n",
    "        labelColName=logistic_label_col_names,\n",
    "        goodValue=logistic_good_value_input,\n",
    "    )\n",
    "\n",
    "    prediction2_step = p.create_step(\"prediction-xflow-maxCompute\",\n",
    "                                        provider=ProviderAlibabaPAI,\n",
    "                                        name=\"prediction2\")\n",
    "    prediction2_step.set_arguments(\n",
    "        inputModelArtifact=logistic_step.outputs[\"outputArtifact\"],\n",
    "        inputDataSetArtifact=split_step.outputs[\"outputArtifact2\"],\n",
    "        execution=execution_input,\n",
    "        outputTableName=\"pai_temp_83935_1099588_1\",\n",
    "        featureColNames=prediction2_feature_col_input,\n",
    "        appendColNames=prediction2_append_col_input,\n",
    "        resultColName=prediction2_result_col_input,\n",
    "        scoreColName=prediction2_score_col_input,\n",
    "        detailColName=prediction2_detail_col_input,\n",
    "    )\n",
    "\n",
    "    evaluate2_step = p.create_step(\"evaluate-xflow-maxCompute\",\n",
    "                                    provider=ProviderAlibabaPAI,\n",
    "                                    name=\"evaluate2\")\n",
    "    evaluate2_step.set_arguments(\n",
    "        inputArtifact=prediction2_step.outputs[\"outputArtifact\"],\n",
    "        execution=execution_input,\n",
    "        outputDetailTableName=\"pai_temp_83935_1099589_1\",\n",
    "        outputMetricTableName=\"pai_temp_83935_1428529_1\",\n",
    "        outputELDetailTableName=\"pai_temp_83935_1199589_1\",\n",
    "        labelColName=evaluate2_label_col_input,\n",
    "        scoreColName=evaluate2_score_col_input,\n",
    "        positiveLabel=evaluate2_positive_label_input,\n",
    "        binCount=evaluate2_bin_count_input,\n",
    "    )\n",
    "\n",
    "    p.create_output_artifact(\"predictionResult\",\n",
    "                                from_=evaluate2_step.outputs[\"outputDetailArtifact\"])\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_pl = create_air_quality_prediction(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_pl.dot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paiflow3",
   "language": "python",
   "name": "paiflow3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}