{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "paiflow3",
   "language": "python",
   "display_name": "paiflow3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction Demo\n",
    "\n",
    "- 这个样例是PAI Studio版本的[心脏病预测案例](https://help.aliyun.com/document_detail/34929.html?spm=a2c4g.11186623.6.769.5b7e340fwAhTsW)的PAIFlow版本，使用了PAIFlow Pipeline service运行心脏病预测案例中的workflow。\n",
    "\n",
    "- 本样例与PAI Studio版本稍有不同，目前的PAIFlow没有提供混淆矩阵的组件，对于结果的评估使用二分类评估器。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化运行环境\n",
    "\n",
    "- 需要提供阿里云的访问密钥对初始化Session:\n",
    "    - https://yuque.antfin-inc.com/pai-user/manual/fqcsry\n",
    "    - 需要使用*set_default_pai_session*(必需)函数，传入AK, 默认的oss_bucket(可选）, 默认的odps_client（可选)初始化默认的session.\n",
    "- 目前的算法模块主要依赖于XFlow实现，数据集和结果数据集大多数情况下是存储在MaxCompute中，因而需要使用MaxCompute访问(ODPS)\n",
    "- 该案例中，PAI算法服务需要访问用户的OSS，提供相应的oss bucket, endpoint, path(存储PMML模型路径), rolearn(PAI服务访问用户OSS的凭证)\n",
    "    - oss rolearn参考 https://help.aliyun.com/document_detail/106225.html?spm=a2c6h.12873639.0.0.82bd6a8a6K624y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import pai\n",
    "print(pai.__file__)\n",
    "\n",
    "import yaml\n",
    "\n",
    "from pai.session import  set_default_pai_session\n",
    "from pai.common import ProviderAlibabaPAI\n",
    "from pai.pipeline import Pipeline\n",
    "from pai.pipeline.template import PipelineTemplate\n",
    "from pai.utils import gen_temp_table\n",
    "from pai.pipeline.types import  PipelineParameter, PipelineArtifact, ArtifactMetadata\n",
    "from pai.pipeline.step import PipelineStep\n",
    "\n",
    "from odps import ODPS, DataFrame\n",
    "import oss2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "##################################### User Config field ##############################\n",
    "config = {\n",
    "    \"access_key_id\": \"your_access_key\",\n",
    "    \"access_key_secret\": \"your_access_key_secret\",\n",
    "    \"region_id\": \"your_region\",\n",
    "    }\n",
    "\n",
    "default_project = \"default_odps_project_name\"\n",
    "\n",
    "\n",
    "oss_endpoint = \"your_oss_endpoint\"\n",
    "# oss_path = \"/paiflow/model_transfer2oss_test/\"\n",
    "oss_path = \"/model_store_path_in_oss/\"\n",
    "oss_bucket_name = \"your_oss_bucket_name\"\n",
    "# OSS Rolearn\n",
    "# 公有云用户参考: https://help.aliyun.com/document_detail/106225.html?spm=a2c6h.12873639.0.0.82bd6a8a6K624y\n",
    "# 集团内参考: https://yuque.antfin-inc.com/pai-user/manual/fqcsry\n",
    "# oss_rolearn = \"acs:ram::{{your_account_id}}:role/aliyunodpspaidefaultrole\"\n",
    "oss_rolearn = \"your_oss_rolearn\"\n",
    "\n",
    "\n",
    "xflow_execution = {\n",
    "    \"odpsInfoFile\": \"/share/base/odpsInfo.ini\",\n",
    "    \"endpoint\": \"ODPS_Endpoint\",\n",
    "    \"logViewHost\": \"logview_host_config\",\n",
    "    \"odpsProject\": default_project,\n",
    "}\n",
    "\n",
    "################################### User Config field ##############################"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "odps_client = ODPS(access_id=config[\"access_key_id\"], secret_access_key=config[\"access_key_secret\"], project=default_project, endpoint=xflow_execution[\"endpoint\"])\n",
    "oss_auth = oss2.Auth(access_key_id=config[\"access_key_id\"], access_key_secret=config[\"access_key_secret\"])\n",
    "oss_bucket = oss2.Bucket(auth=oss_auth, endpoint=oss_endpoint, bucket_name=oss_bucket_name)\n",
    "\n",
    "session = set_default_pai_session(odps_client=odps_client, oss_bucket=oss_bucket, **config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 获取可用的PipelineTemplate\n",
    "\n",
    "PipelineTemplate表示Pipeline或是Component的*定义*， 可以是保存在服务端的模板， 也可以是由本地构造/拼接获得的组件的定义，区别是前者包含一个由服务端生成的PipelineId标识，后者可以通过save接口保存到服务端，并且获得对应的PipelineId。\n",
    "\n",
    "### PipelineTemplate class可以通过:\n",
    "\n",
    "- list_templates(identifier, provider, version, fuzzy):\n",
    "    - 搜索/列出获得已保存的PAIFlow.\n",
    "\n",
    "- get(pipeline_id)/get_by_identifier(identifier, provider, version):\n",
    "    - 获得具体的某一个Pipeline模板\n",
    "\n",
    "\n",
    "### PipelineTemplate对象主要支持：\n",
    "\n",
    "- load():\n",
    "    - 尝试获得Template的实现（调用DescribePipeline接口获得详细的Implementation），解析实现获得对应的Component或是Pipeline对象.\n",
    "\n",
    "- run(job_name, arguments):\n",
    "    - 传入运行参数直接运行任务\n",
    "\n",
    "- inputs/outputs:\n",
    "    - property: 获取查看模板的inputs/outputs信息\n",
    "\n",
    "- save(identifier, version):\n",
    "    - 指定identifier和version，保存当前的Manifest\n",
    "\n",
    "- as_step(inputs, name)：\n",
    "    - 生成一个PipelineStep,可以用于Pipeline拼接\n",
    "    - 注：目前PAIFlow的后端只支持使用*已保存*template\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates, count = PipelineTemplate.list_templates(identifier=\"xflow\", provider=ProviderAlibabaPAI, fuzzy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "templates[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates[0].inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-By-Step的方式运行任务\n",
    "\n",
    "- 使用同步方式，提交任务，获得返回结果后，使用结果数据集提交给下一个任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用的数据集\n",
    "数据集使用的是PAI提供的公共读MaxCompute table- heart_disease_prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 公共读的数据表项目\n",
    "# 集团内是pai_inner_project;\n",
    "# 集团外（公有云）是pai_online_project\n",
    "\n",
    "source_table_project = \"pai_inner_project\"\n",
    "\n",
    "dataset_table = \"odps://{}/tables/heart_disease_prediction\".format(source_table_project)\n",
    "\n",
    "odps_table = odps_client.get_table(\"heart_disease_prediction\", project=source_table_project)\n",
    "df = DataFrame(odps_table)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理\n",
    "- 将字符串数据转换为数值类型\n",
    "- run中传入的参数parameters dict，key表示的是对应的Manifest的inputs的name， value则表示需要输入的数据."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sql = \"select age, (case sex when 'male' then 1 else 0 end) as sex, (case cp when \" \\\n",
    "        \"'angina' then 0  when 'notang' then 1 else 2 end) as cp, trestbps, chol, (case\" \\\n",
    "        \" fbs when 'true' then 1 else 0 end) as fbs, (case restecg when 'norm' then 0 \" \\\n",
    "        \" when 'abn' then 1 else 2 end) as restecg, thalach, (case exang when 'true' then\" \\\n",
    "        \" 1 else 0 end) as exang, oldpeak, (case slop when 'up' then 0  when 'flat' then \" \\\n",
    "        \"1 else 2 end) as slop, ca, (case thal when 'norm' then 0  when 'fix' then 1\" \\\n",
    "        \" else 2 end) as thal, (case status  when 'sick' then 1 else 0 end) as\" \\\n",
    "        \" ifHealth from ${t1};\"\n",
    "\n",
    "# Extract and transform dataset using max_compute sql.\n",
    "sql_job = PipelineTemplate.get_by_identifier(\n",
    "    identifier=\"sql-xflow-maxCompute\", provider=ProviderAlibabaPAI, version=\"v1\").run(\n",
    "    job_name=\"sql-job\",\n",
    "    arguments={\n",
    "        \"execution\": xflow_execution,\n",
    "        \"inputArtifact1\": dataset_table,\n",
    "        \"sql\": sql,\n",
    "        \"outputTable\": gen_temp_table(),\n",
    "    })\n",
    "\n",
    "# because of outputs not ready after workflow finished.\n",
    "time.sleep(10)\n",
    "output_table_artifact = sql_job.get_outputs()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "sql_output_table = odps_client.get_table(output_table_artifact.value.table, output_table_artifact.value.project)\n",
    "DataFrame(sql_output_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Heart-disease-prediction的工作流包括以下\n",
    "\n",
    "- type_transform 完成数据类型转换，将部分列装换为doubel\n",
    "- normalize_job 完成数据归一化处理\n",
    "- split_job 切分数据为训练数据集和验证数据集\n",
    "- lr_job 使用训练数据集训练，获得一个offlineModel\n",
    "- transform_job 使用offlinemodel和验证数据集进行批量预测\n",
    "- evaluate_job 使用预测结果评估模型准确性\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pai.xflow.classifier import LogisticRegression\n",
    "# Transform value to Double\n",
    "type_transform_job = PipelineTemplate.get_by_identifier(\n",
    "    identifier=\"type-transform-xflow-maxCompute\",\n",
    "    provider=ProviderAlibabaPAI, version=\"v1\").run(\n",
    "    job_name=\"type-transform-job\",\n",
    "    arguments={\n",
    "        \"execution\": xflow_execution,\n",
    "        \"inputArtifact\": output_table_artifact,\n",
    "        \"cols_to_double\": 'sex,cp,fbs,restecg,exang,slop,thal,ifhealth,age,trestbps,chol,thalach,oldpeak,ca',\n",
    "        \"outputTable\": gen_temp_table(),\n",
    "    }\n",
    ")\n",
    "\n",
    "time.sleep(20)\n",
    "type_transform_result = type_transform_job.get_outputs()[0]\n",
    "\n",
    "# Normalize Feature\n",
    "normalize_job = PipelineTemplate.get_by_identifier(identifier=\"normalize-xflow-maxCompute\",\n",
    "                                            provider=ProviderAlibabaPAI, version=\"v1\").run(\n",
    "    job_name=\"normalize-job\",\n",
    "    arguments={\n",
    "        \"execution\": xflow_execution,\n",
    "        \"inputArtifact\": type_transform_result,\n",
    "        \"selectedColNames\": 'sex,cp,fbs,restecg,exang,slop,thal,ifhealth,age,trestbps,'\n",
    "                            'chol,thalach,oldpeak,ca',\n",
    "        \"lifecycle\": 1,\n",
    "        \"outputTableName\": gen_temp_table(),\n",
    "        \"outputParaTableName\": gen_temp_table(),\n",
    "    }\n",
    ")\n",
    "\n",
    "# because of outputs is not ready after run is succeeded.\n",
    "time.sleep(20)\n",
    "normalized_dataset = normalize_job.get_outputs()[0]\n",
    "\n",
    "split_job = PipelineTemplate.get_by_identifier(\n",
    "    identifier=\"split-xflow-maxCompute\",\n",
    "    provider=ProviderAlibabaPAI, version=\"v1\").run(\n",
    "    job_name=\"split-job\",\n",
    "    arguments={\n",
    "        \"inputArtifact\": normalized_dataset,\n",
    "        \"execution\": xflow_execution,\n",
    "        \"fraction\": 0.8,\n",
    "        \"output1TableName\": gen_temp_table(),\n",
    "        \"output2TableName\": gen_temp_table(),\n",
    "    }\n",
    ")\n",
    "\n",
    "time.sleep(20)\n",
    "split_output_1, split_output_2 = split_job.get_outputs()\n",
    "\n",
    "\n",
    "lr_job = LogisticRegression(\n",
    "    regularized_type=\"l2\", xflow_execution=xflow_execution,\n",
    "    pmml_gen=True, pmml_oss_bucket=oss_bucket_name,\n",
    "    pmml_oss_path=oss_path, pmml_oss_endpoint=oss_endpoint,\n",
    "    pmml_oss_rolearn=oss_rolearn,\n",
    ").fit(split_output_1,\n",
    "        wait=True,\n",
    "        feature_cols='sex,cp,fbs,restecg,exang,slop,thal,age,trestbps,chol,thalach,oldpeak,ca',\n",
    "        label_col=\"ifhealth\",\n",
    "        good_value=1,\n",
    "        model_name=\"test_health_prediction\")\n",
    "\n",
    "time.sleep(20)\n",
    "offlinemodel_artifact, pmml_output = lr_job.get_outputs()\n",
    "transform_job = PipelineTemplate.get_by_identifier(\n",
    "    identifier=\"prediction-xflow-maxCompute\",\n",
    "    provider=ProviderAlibabaPAI, version=\"v1\").run(\n",
    "    job_name=\"prediction-job\",\n",
    "    arguments={\n",
    "        \"inputModelArtifact\": offlinemodel_artifact,\n",
    "        \"inputDataSetArtifact\": split_output_2,\n",
    "        \"execution\": xflow_execution,\n",
    "        \"outputTableName\": gen_temp_table(),\n",
    "        \"featureColNames\": 'sex,cp,fbs,restecg,exang,slop,thal,age,trestbps,chol,thalach,oldpeak,ca',\n",
    "        \"appendColNames\": \"ifhealth\",\n",
    "    }\n",
    ")\n",
    "\n",
    "time.sleep(20)\n",
    "transform_result = transform_job.get_outputs()[0]\n",
    "\n",
    "evaluate_job = PipelineTemplate.get_by_identifier(\n",
    "    identifier=\"evaluate-xflow-maxCompute\",\n",
    "    provider=ProviderAlibabaPAI, version=\"v1\",\n",
    "    ).run(\n",
    "    job_name=\"evaluate-job\",\n",
    "    arguments={\n",
    "        \"execution\": xflow_execution,\n",
    "        \"inputArtifact\": transform_result,\n",
    "        \"outputDetailTableName\": gen_temp_table(),\n",
    "        \"outputELDetailTableName\": gen_temp_table(),\n",
    "        \"outputMetricTableName\": gen_temp_table(),\n",
    "        \"scoreColName\": \"prediction_score\",\n",
    "        \"labelColName\": \"ifhealth\",\n",
    "        \"coreNum\": 2,\n",
    "        \"memSizePerCore\": 512,\n",
    "    }\n",
    ")\n",
    "\n",
    "time.sleep(20)\n",
    "evaluate_result = evaluate_job.get_outputs()[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看训练效果评估数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "evaluate_table = odps_client.get_table(evaluate_result.value.table, evaluate_result.value.project)\n",
    "DataFrame(evaluate_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上的case是一个step-by-step的任务，节点任务之间的串联是维护在具体的Python code中，不利于复杂Pipeline的分享复用，以及执行的优化。\n",
    "PAIFlow提供了将节点串联，构建为一个复合Pipeline的能力。\n",
    "\n",
    "## 复合Pipeline的构建\n",
    "\n",
    "\n",
    "- 使用PAI提供的Pipeline实现（包括，split, normalize, logisticregression, prediction, evaluate等）,构建出一个复合Pipeline工作定义。\n",
    "- 使用函数构建复合Pipeline：构建过程中出现的错误操作，不会污染notebook kernel的全局环境（除非对global变量进行写入），如果需要修改构建Pipeline的流程，例如修改参数名，减少参数，则只需要修改函数定义，重新运行函数获得新的Pipeline定义\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pai.pipeline.types import ArtifactDataType, ArtifactLocationType\n",
    "def create_heart_disease_pred_pl():\n",
    "    pmml_oss_bucket = PipelineParameter(\"pmml_oss_bucket\")\n",
    "    pmml_oss_rolearn = PipelineParameter(\"pmml_oss_rolearn\")\n",
    "    pmml_oss_path = PipelineParameter(\"pmml_oss_path\")\n",
    "    pmml_oss_endpoint = PipelineParameter(\"pmml_oss_endpoint\")\n",
    "    xflow_execution = PipelineParameter(\"xflow_execution\", dict)\n",
    "    dataset_input = PipelineArtifact(\"dataset-table\",\n",
    "                                        metadata=ArtifactMetadata(\n",
    "                                            data_type=ArtifactDataType.DataSet,\n",
    "                                            location_type=ArtifactLocationType.MaxComputeTable,\n",
    "                                        ),\n",
    "                                        required=True)\n",
    "\n",
    "    sql = \"select age, (case sex when 'male' then 1 else 0 end) as sex,(case cp when\" \\\n",
    "            \" 'angina' then 0  when 'notang' then 1 else 2 end) as cp, trestbps, chol,\" \\\n",
    "            \" (case fbs when 'true' then 1 else 0 end) as fbs, (case restecg when 'norm'\" \\\n",
    "            \" then 0  when 'abn' then 1 else 2 end) as restecg, thalach, (case exang when\" \\\n",
    "            \" 'true' then 1 else 0 end) as exang, oldpeak, (case slop when 'up' then 0  \" \\\n",
    "            \"when 'flat' then 1 else 2 end) as slop, ca, (case thal when 'norm' then 0 \" \\\n",
    "            \" when 'fix' then 1 else 2 end) as thal, (case status when 'sick' then 1 else \" \\\n",
    "            \"0 end) as ifHealth from ${t1};\"\n",
    "    sql_step = PipelineStep(\"sql-xflow-maxCompute\", name=\"sql-1\",\n",
    "                            provider=ProviderAlibabaPAI,\n",
    "                            version=\"v1\",\n",
    "                            inputs={\n",
    "                                \"inputArtifact1\": dataset_input,\n",
    "                                \"execution\": xflow_execution,\n",
    "                                \"sql\": sql,\n",
    "                                \"outputTable\": gen_temp_table(),\n",
    "                            })\n",
    "\n",
    "    type_transform_step = PipelineStep(\n",
    "        \"type-transform-xflow-maxCompute\",\n",
    "        name=\"type-transform-1\",\n",
    "        provider=ProviderAlibabaPAI, version=\"v1\",\n",
    "        inputs={\n",
    "            \"execution\": xflow_execution,\n",
    "            \"inputArtifact\": sql_step.outputs[\"outputArtifact\"],\n",
    "            \"cols_to_double\": 'sex,cp,fbs,restecg,exang,slop,thal,ifhealth,age,trestbps,'\n",
    "                                'chol,thalach,oldpeak,ca',\n",
    "            \"outputTable\": gen_temp_table(),\n",
    "\n",
    "        })\n",
    "\n",
    "    normalize_step = PipelineStep(\n",
    "        \"normalize-xflow-maxCompute\",\n",
    "        name=\"normalize-1\",\n",
    "        provider=ProviderAlibabaPAI,\n",
    "        version=\"v1\", inputs={\n",
    "            \"execution\": xflow_execution,\n",
    "            \"inputArtifact\": type_transform_step.outputs[\"outputArtifact\"],\n",
    "            \"selectedColNames\": 'sex,cp,fbs,restecg,exang,slop,thal,ifhealth,age,trestbps,'\n",
    "                                'chol,thalach,oldpeak,ca',\n",
    "            \"lifecycle\": 1,\n",
    "            \"outputTableName\": gen_temp_table(),\n",
    "            \"outputParaTableName\": gen_temp_table(),\n",
    "\n",
    "        })\n",
    "\n",
    "    split_step = PipelineStep(\n",
    "        identifier=\"split-xflow-maxCompute\",\n",
    "        name='split-1',\n",
    "        provider=ProviderAlibabaPAI, version=\"v1\", inputs={\n",
    "            \"inputArtifact\": normalize_step.outputs[\"outputArtifact\"],\n",
    "            \"execution\": xflow_execution,\n",
    "            \"fraction\": 0.8,\n",
    "            \"output1TableName\": gen_temp_table(),\n",
    "            \"output2TableName\": gen_temp_table(),\n",
    "\n",
    "        }\n",
    "    )\n",
    "\n",
    "    model_name = 'test_health_prediction_by_pipeline_%s' % (random.randint(0, 999999))\n",
    "\n",
    "    lr_step = PipelineStep(\n",
    "        identifier=\"logisticregression-binary-xflow-maxCompute\",\n",
    "        name=\"logisticregression-1\",\n",
    "        provider=ProviderAlibabaPAI, version=\"v1\", inputs={\n",
    "            \"inputArtifact\": split_step.outputs[\"outputArtifact1\"],\n",
    "            \"execution\": xflow_execution,\n",
    "            \"generatePmml\": True,\n",
    "            \"endpoint\": pmml_oss_endpoint,\n",
    "            \"bucket\": pmml_oss_bucket,\n",
    "            \"path\": pmml_oss_path,\n",
    "            \"rolearn\": pmml_oss_rolearn,\n",
    "            # \"regulizedType\": \"l2\",\n",
    "            \"modelName\": model_name,\n",
    "            \"goodValue\": 1,\n",
    "            \"featureColNames\": \"sex,cp,fbs,restecg,exang,slop,thal,age,trestbps,chol,thalach,oldpeak,ca\",\n",
    "            \"labelColName\": \"ifhealth\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    offline_model_pred_step = PipelineStep(\n",
    "        identifier=\"prediction-xflow-maxCompute\",\n",
    "        name=\"offlinemodel-pred\",\n",
    "        provider=ProviderAlibabaPAI, version=\"v1\", inputs={\n",
    "            \"inputModelArtifact\": lr_step.outputs[\"outputArtifact\"],\n",
    "            \"inputDataSetArtifact\": split_step.outputs[\"outputArtifact2\"],\n",
    "            \"execution\": xflow_execution,\n",
    "            \"outputTableName\": gen_temp_table(),\n",
    "            \"featureColNames\": 'sex,cp,fbs,restecg,exang,slop,thal,age,trestbps,chol,thalach,oldpeak,ca',\n",
    "            \"appendColNames\": \"ifhealth\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    evaluate_step = PipelineStep(\n",
    "        identifier=\"evaluate-xflow-maxCompute\",\n",
    "        name=\"evaluate-1\",\n",
    "        provider=ProviderAlibabaPAI, version=\"v1\",\n",
    "        inputs={\n",
    "            \"execution\": xflow_execution,\n",
    "            \"inputArtifact\": offline_model_pred_step.outputs[\"outputArtifact\"],\n",
    "            \"outputDetailTableName\": gen_temp_table(),\n",
    "            \"outputELDetailTableName\": gen_temp_table(),\n",
    "            \"outputMetricTableName\": gen_temp_table(),\n",
    "            \"scoreColName\": \"prediction_score\",\n",
    "            \"labelColName\": \"ifhealth\",\n",
    "            \"coreNum\": 2,\n",
    "            \"memSizePerCore\": 512,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    p = Pipeline(\n",
    "        steps=[evaluate_step, offline_model_pred_step],\n",
    "        outputs={\"pmmlModel\": lr_step.outputs[\"outputArtifact\"],\n",
    "                    \"evaluateResult\": evaluate_step.outputs[\"outputMetricsArtifact\"]\n",
    "                    }\n",
    "    )\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建Pipeline\n",
    "- 可以使用`pipeline.dot()`命令查看Pipeline的运行拓扑图(需要安装graphviz)\n",
    "- 可以通过p.to_dict()方法获得对应的Pipeline Manifest的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "p = create_heart_disease_pred_pl()\n",
    "print(yaml.dump(p.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "p.dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 运行Pipeline\n",
    "- 指定输入参数，运行Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "run_instance = p.run(job_name=\"heart-disease-pipeline-job\", arguments={\n",
    "    \"dataset-table\": dataset_table,\n",
    "    \"xflow_execution\": xflow_execution,\n",
    "    \"pmml_oss_endpoint\": oss_endpoint,\n",
    "    \"pmml_oss_bucket\": oss_bucket_name,\n",
    "    \"pmml_oss_path\": oss_path,\n",
    "    \"pmml_oss_rolearn\": oss_rolearn,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "run_instance.get_outputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将Pipeline推送到服务端保存，进行复用\n",
    "- 保存的Pipeline 可以作为其他Pipeline的节点进行复用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "p.save(identifier=\"heart-disease-prediction-pl\", version=\"v%s\"%int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "save_pipeline = PipelineTemplate.get(p.pipeline_id).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_pipeline.dot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "saved_pipeline = PipelineTemplate.get(p.pipeline_id)\n",
    "saved_pipeline.run(job_name=\"saved-pl-heart-disease-job\", arguments={\n",
    "    \"dataset-table\": dataset_table,\n",
    "    \"xflow_execution\": xflow_execution,\n",
    "    \"pmml_oss_endpoint\": oss_endpoint,\n",
    "    \"pmml_oss_bucket\": oss_bucket_name,\n",
    "    \"pmml_oss_path\": oss_path,\n",
    "    \"pmml_oss_rolearn\": oss_rolearn,\n",
    "})"
   ]
  }
 ]
}